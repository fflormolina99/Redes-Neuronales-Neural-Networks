{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "#from torch.utils.tensorboard import SummaryWriter # TensorBoard support\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import time\n",
    "#from torchviz import make_dot\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import dill\n",
    "import json\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)     # On by default, leave it here for clarity\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import files  \n",
    "    COLAB = True\n",
    "except:\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Las cosas que vamos a variar son:\n",
    "- Funcion de activacion: vamos a variar entre ReLU y Leaky ReLU\n",
    "- Dropout: vamos a usar p=0.1 y p=0.3\n",
    "- Optimizador: vamos a usar ADAM y SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando el dispositivo cpu\n"
     ]
    }
   ],
   "source": [
    "#Como siempre chequeamos que este el GPU o el CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Usando el dispositivo {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos de FASHION-MNIST\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network, expand on top of nn.Module\n",
    "class Network(nn.Module):\n",
    "  def __init__(self,p=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    # define layers\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "    \n",
    "    \n",
    "    self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "    self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "    self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    \n",
    "    self.dropout = nn.Dropout(p)\n",
    "\n",
    "  # define forward function\n",
    "  def forward(self, t):\n",
    "    # conv 1\n",
    "    t = self.conv1(t)\n",
    "    t = F.relu(t)\n",
    "    t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "    # conv 2\n",
    "    t = self.conv2(t)\n",
    "    t = F.relu(t)\n",
    "    t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "    # fc1\n",
    "    t = t.reshape(-1, 12*4*4)\n",
    "    t = self.fc1(t)\n",
    "    t = self.dropout(t)\n",
    "    t = F.relu(t)\n",
    "\n",
    "    # fc2\n",
    "    t = self.fc2(t)\n",
    "    t = self.dropout(t)\n",
    "    t = F.relu(t)\n",
    "\n",
    "    # output\n",
    "    t = self.out(t)\n",
    "    # don't need softmax here since we'll use cross-entropy as activation.\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Network().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función de entrenamiento\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # Iteramos sobre lotes (batchs)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Calculamos la predicción del modelo y la correspondiente pérdida (error)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagamos usando el optimizador proveido.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Imprimimos el progreso...\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            #print(f\"batch={batch} loss={loss:>7f}  muestras-procesadas:[{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# De manera similar, definimos la función de testeo\n",
    "def test_loop(dataloader,model,loss_fn):\n",
    "    num_samples  = 0\n",
    "    num_batches  = 0\n",
    "    avrg_loss    = 0\n",
    "    frac_correct = 0\n",
    "    # Seteamos el modelo en modo evaluacion. Esto sirve para desactivar, por ejemplo, dropout, etc. cuando no estamos en una fase de entrenamiento.\n",
    "    model.eval()\n",
    "    # Pasamos el modelo la GPU si está disponible.    \n",
    "    model = model.to(device)    \n",
    "    # Para validar, desactivamos el cálculo de gradientes.\n",
    "    with torch.no_grad():\n",
    "        # Iteramos sobre lotes (batches)\n",
    "        for X,y in dataloader:\n",
    "            # Pasamos los tensores a la GPU si está disponible.\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)           \n",
    "            # Calculamos las predicciones del modelo...\n",
    "            pred = model(X)\n",
    "            # y las correspondientes pérdidas (errores), los cuales vamos acumulando en un valor total.\n",
    "            num_batches += 1\n",
    "            avrg_loss += loss_fn(pred,y).item()\n",
    "            # También calculamos el número de predicciones correctas, y lo acumulamos en un total.\n",
    "            num_samples += y.size(0)            \n",
    "            frac_correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    # Calculamos la pérdida total y la fracción de clasificaciones correctas, y las imprimimos.\n",
    "    avrg_loss    /= num_batches\n",
    "    frac_correct /= num_samples\n",
    "    #print(f\"Test Error: \\n Accuracy: {frac_correct:>0.5f}, Avg. loss: {avrg_loss:>8f} \\n\")\n",
    "    return avrg_loss,frac_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=10 p=0.1 epoch=0 train_loss=0.8500287334124247 train_accu=0.6877666666666666 valid_loss=0.864807802438736 valid_accu=0.684\n",
      "n=10 p=0.1 epoch=1 train_loss=0.6948346416155498 train_accu=0.73495 valid_loss=0.7132699728012085 valid_accu=0.7292\n",
      "n=10 p=0.1 epoch=2 train_loss=0.6310261984666189 train_accu=0.7567666666666667 valid_loss=0.6497359275817871 valid_accu=0.7509\n",
      "n=10 p=0.1 epoch=3 train_loss=0.5853830521305402 train_accu=0.7768833333333334 valid_loss=0.6044900238513946 valid_accu=0.7689\n",
      "n=10 p=0.1 epoch=4 train_loss=0.5507020577788353 train_accu=0.79415 valid_loss=0.5703883707523346 valid_accu=0.7833\n",
      "n=10 p=0.1 epoch=5 train_loss=0.5209681098659833 train_accu=0.8065666666666667 valid_loss=0.5422378063201905 valid_accu=0.7965\n",
      "n=10 p=0.1 epoch=6 train_loss=0.4975489467382431 train_accu=0.8167166666666666 valid_loss=0.5199748277664185 valid_accu=0.8059\n",
      "n=10 p=0.1 epoch=7 train_loss=0.47849336365858713 train_accu=0.8248833333333333 valid_loss=0.5017484813928604 valid_accu=0.8138\n",
      "n=10 p=0.1 epoch=8 train_loss=0.4606506327788035 train_accu=0.8307666666666667 valid_loss=0.48395727276802064 valid_accu=0.8239\n",
      "n=10 p=0.1 epoch=9 train_loss=0.4452833577990532 train_accu=0.83715 valid_loss=0.4690522700548172 valid_accu=0.8289\n",
      "n=10 p=0.1 epoch=10 train_loss=0.4302267551422119 train_accu=0.84435 valid_loss=0.4547059267759323 valid_accu=0.8379\n",
      "n=10 p=0.1 epoch=11 train_loss=0.4129773403207461 train_accu=0.8516 valid_loss=0.4389108568429947 valid_accu=0.8419\n",
      "n=10 p=0.1 epoch=12 train_loss=0.401099206507206 train_accu=0.85575 valid_loss=0.4278710901737213 valid_accu=0.8466\n",
      "n=10 p=0.1 epoch=13 train_loss=0.3908898696303368 train_accu=0.8594 valid_loss=0.4195100635290146 valid_accu=0.8508\n",
      "n=10 p=0.1 epoch=14 train_loss=0.3835253496964773 train_accu=0.86105 valid_loss=0.41323972046375274 valid_accu=0.8539\n",
      "n=10 p=0.1 epoch=15 train_loss=0.3763618012269338 train_accu=0.8632 valid_loss=0.4073852330446243 valid_accu=0.8561\n",
      "n=10 p=0.1 epoch=16 train_loss=0.36681463569402695 train_accu=0.8667166666666667 valid_loss=0.3984597474336624 valid_accu=0.859\n",
      "n=10 p=0.1 epoch=17 train_loss=0.3584075863162676 train_accu=0.8691 valid_loss=0.39080305993556974 valid_accu=0.8617\n",
      "n=10 p=0.1 epoch=18 train_loss=0.35111325681209565 train_accu=0.8716 valid_loss=0.38388784527778624 valid_accu=0.863\n",
      "n=10 p=0.1 epoch=19 train_loss=0.34197091311216354 train_accu=0.87495 valid_loss=0.3750273108482361 valid_accu=0.8661\n",
      "n=10 p=0.1 epoch=20 train_loss=0.33403994888067245 train_accu=0.87725 valid_loss=0.3667854696512222 valid_accu=0.8691\n",
      "n=10 p=0.1 epoch=21 train_loss=0.32932660380999246 train_accu=0.8783833333333333 valid_loss=0.36202537417411806 valid_accu=0.8717\n",
      "n=10 p=0.1 epoch=22 train_loss=0.3259187027812004 train_accu=0.8805 valid_loss=0.3587059140205383 valid_accu=0.871\n",
      "n=10 p=0.1 epoch=23 train_loss=0.32241488148768743 train_accu=0.88145 valid_loss=0.3557270228862762 valid_accu=0.8702\n",
      "n=10 p=0.1 epoch=24 train_loss=0.3177507549524307 train_accu=0.88305 valid_loss=0.35227854549884796 valid_accu=0.8726\n",
      "n=10 p=0.1 epoch=25 train_loss=0.3139327128728231 train_accu=0.8842166666666667 valid_loss=0.34948801100254057 valid_accu=0.8747\n",
      "n=10 p=0.1 epoch=26 train_loss=0.31035687973101933 train_accu=0.8852166666666667 valid_loss=0.34699287712574006 valid_accu=0.8762\n",
      "n=10 p=0.1 epoch=27 train_loss=0.30646315962076187 train_accu=0.8864666666666666 valid_loss=0.3441499412059784 valid_accu=0.8763\n",
      "n=10 p=0.1 epoch=28 train_loss=0.3037568137049675 train_accu=0.8874666666666666 valid_loss=0.3423775196075439 valid_accu=0.8758\n",
      "n=10 p=0.1 epoch=29 train_loss=0.3004279941320419 train_accu=0.88875 valid_loss=0.3401931434869766 valid_accu=0.8775\n",
      "n=10 p=0.1 epoch=30 train_loss=0.2974776834249496 train_accu=0.8900666666666667 valid_loss=0.3385279357433319 valid_accu=0.8781\n",
      "n=10 p=0.1 epoch=31 train_loss=0.29436539063851036 train_accu=0.89165 valid_loss=0.33653227984905243 valid_accu=0.8786\n",
      "n=10 p=0.1 epoch=32 train_loss=0.29251528705159824 train_accu=0.8918833333333334 valid_loss=0.3357017427682877 valid_accu=0.8783\n",
      "n=10 p=0.1 epoch=33 train_loss=0.2901677958667278 train_accu=0.8935666666666666 valid_loss=0.3344056338071823 valid_accu=0.8796\n",
      "n=10 p=0.1 epoch=34 train_loss=0.28808192238211633 train_accu=0.8944 valid_loss=0.3335160851478577 valid_accu=0.8801\n",
      "n=10 p=0.1 epoch=35 train_loss=0.28549687216679254 train_accu=0.8952833333333333 valid_loss=0.33180093169212344 valid_accu=0.8817\n",
      "n=10 p=0.1 epoch=36 train_loss=0.2828275509178638 train_accu=0.8963166666666667 valid_loss=0.33005708158016206 valid_accu=0.8819\n",
      "n=10 p=0.1 epoch=37 train_loss=0.2807367108762264 train_accu=0.897 valid_loss=0.32899933457374575 valid_accu=0.8817\n",
      "n=10 p=0.1 epoch=38 train_loss=0.2790081756810347 train_accu=0.8975166666666666 valid_loss=0.32804052233695985 valid_accu=0.8829\n",
      "n=10 p=0.1 epoch=39 train_loss=0.2768014440933863 train_accu=0.8985166666666666 valid_loss=0.326859250664711 valid_accu=0.8827\n",
      "n=10 p=0.1 epoch=40 train_loss=0.2748669800659021 train_accu=0.8991666666666667 valid_loss=0.3258174151182175 valid_accu=0.8826\n",
      "n=10 p=0.1 epoch=41 train_loss=0.27295412744084996 train_accu=0.89955 valid_loss=0.32488738000392914 valid_accu=0.8825\n",
      "n=10 p=0.1 epoch=42 train_loss=0.27114545529087386 train_accu=0.9002 valid_loss=0.3241226077079773 valid_accu=0.8833\n",
      "n=10 p=0.1 epoch=43 train_loss=0.2686912305653095 train_accu=0.9011 valid_loss=0.3226288467645645 valid_accu=0.8828\n",
      "n=10 p=0.1 epoch=44 train_loss=0.26610475704073905 train_accu=0.90225 valid_loss=0.32156423628330233 valid_accu=0.8839\n",
      "n=10 p=0.1 epoch=45 train_loss=0.2643639400601387 train_accu=0.9030333333333334 valid_loss=0.3205263912677765 valid_accu=0.8842\n",
      "n=10 p=0.1 epoch=46 train_loss=0.26217171202103295 train_accu=0.9033666666666667 valid_loss=0.3193109631538391 valid_accu=0.8854\n",
      "n=10 p=0.1 epoch=47 train_loss=0.26069758236408236 train_accu=0.9037833333333334 valid_loss=0.31872196197509767 valid_accu=0.8859\n",
      "n=10 p=0.1 epoch=48 train_loss=0.25892647604147595 train_accu=0.9045833333333333 valid_loss=0.31777205765247346 valid_accu=0.8859\n",
      "n=10 p=0.1 epoch=49 train_loss=0.25761982848246895 train_accu=0.9048166666666667 valid_loss=0.31730506420135496 valid_accu=0.8866\n",
      "n=10 p=0.1 epoch=50 train_loss=0.2572280024488767 train_accu=0.9052333333333333 valid_loss=0.317950239777565 valid_accu=0.8872\n",
      "n=10 p=0.1 epoch=51 train_loss=0.2560414853195349 train_accu=0.90595 valid_loss=0.3172649621963501 valid_accu=0.8873\n",
      "n=10 p=0.1 epoch=52 train_loss=0.25469094663858416 train_accu=0.906 valid_loss=0.3168551027774811 valid_accu=0.8887\n",
      "n=10 p=0.1 epoch=53 train_loss=0.2550430861612161 train_accu=0.9063333333333333 valid_loss=0.3177103579044342 valid_accu=0.8894\n",
      "n=10 p=0.1 epoch=54 train_loss=0.2549894767502944 train_accu=0.9063833333333333 valid_loss=0.31849945783615113 valid_accu=0.8892\n",
      "n=10 p=0.1 epoch=55 train_loss=0.2548365692297618 train_accu=0.90655 valid_loss=0.31879437267780303 valid_accu=0.8891\n",
      "n=10 p=0.1 epoch=56 train_loss=0.25540660371383034 train_accu=0.9059166666666667 valid_loss=0.3194896370172501 valid_accu=0.8891\n",
      "n=10 p=0.1 epoch=57 train_loss=0.2527360146244367 train_accu=0.9069 valid_loss=0.3180853843688965 valid_accu=0.8896\n",
      "n=10 p=0.1 epoch=58 train_loss=0.25126323451598487 train_accu=0.9073833333333333 valid_loss=0.31801897287368774 valid_accu=0.89\n",
      "n=10 p=0.1 epoch=59 train_loss=0.24870269025365513 train_accu=0.9084833333333333 valid_loss=0.31704006195068357 valid_accu=0.8911\n",
      "n=10 p=0.1 epoch=60 train_loss=0.2471795154114564 train_accu=0.9088666666666667 valid_loss=0.31655019521713257 valid_accu=0.8913\n",
      "n=10 p=0.1 epoch=61 train_loss=0.24595849663019181 train_accu=0.90905 valid_loss=0.31671961545944216 valid_accu=0.891\n",
      "n=10 p=0.1 epoch=62 train_loss=0.24400518859426182 train_accu=0.9102 valid_loss=0.3156877249479294 valid_accu=0.8916\n",
      "n=10 p=0.1 epoch=63 train_loss=0.24309856419761974 train_accu=0.9103666666666667 valid_loss=0.3159110099077225 valid_accu=0.8907\n",
      "n=10 p=0.1 epoch=64 train_loss=0.24065908392270405 train_accu=0.91115 valid_loss=0.3148170977830887 valid_accu=0.8913\n",
      "n=10 p=0.1 epoch=65 train_loss=0.23862061128020287 train_accu=0.91155 valid_loss=0.3139786034822464 valid_accu=0.8916\n",
      "n=10 p=0.1 epoch=66 train_loss=0.23741602376103402 train_accu=0.9120666666666667 valid_loss=0.313779816031456 valid_accu=0.8915\n",
      "n=10 p=0.1 epoch=67 train_loss=0.2344900707403819 train_accu=0.9131666666666667 valid_loss=0.3126765936613083 valid_accu=0.892\n",
      "n=10 p=0.1 epoch=68 train_loss=0.23380826165278754 train_accu=0.9134666666666666 valid_loss=0.31319176554679873 valid_accu=0.892\n",
      "n=10 p=0.1 epoch=69 train_loss=0.23194067155321438 train_accu=0.9138833333333334 valid_loss=0.31298250555992124 valid_accu=0.8923\n",
      "n=10 p=0.1 epoch=70 train_loss=0.23091262032588322 train_accu=0.9142333333333333 valid_loss=0.3127832531929016 valid_accu=0.8919\n",
      "n=10 p=0.1 epoch=71 train_loss=0.22890659297506014 train_accu=0.9147833333333333 valid_loss=0.3124510020017624 valid_accu=0.8919\n",
      "n=10 p=0.1 epoch=72 train_loss=0.22743169839183489 train_accu=0.91555 valid_loss=0.31229738891124725 valid_accu=0.8921\n",
      "n=10 p=0.1 epoch=73 train_loss=0.2263343947629134 train_accu=0.9157 valid_loss=0.3117451637983322 valid_accu=0.892\n",
      "n=10 p=0.1 epoch=74 train_loss=0.22485006724794707 train_accu=0.91605 valid_loss=0.31200979351997377 valid_accu=0.892\n",
      "n=10 p=0.1 epoch=75 train_loss=0.22359642262260118 train_accu=0.91645 valid_loss=0.31207587122917174 valid_accu=0.8913\n",
      "n=10 p=0.1 epoch=76 train_loss=0.22257205173373223 train_accu=0.9172166666666667 valid_loss=0.31246896386146544 valid_accu=0.8909\n",
      "n=10 p=0.1 epoch=77 train_loss=0.2216347557802995 train_accu=0.9175666666666666 valid_loss=0.3126895070075989 valid_accu=0.8903\n",
      "n=10 p=0.1 epoch=78 train_loss=0.22008105367422104 train_accu=0.9181833333333334 valid_loss=0.3124574512243271 valid_accu=0.8912\n",
      "n=10 p=0.1 epoch=79 train_loss=0.2194024220108986 train_accu=0.91825 valid_loss=0.31317917108535764 valid_accu=0.8908\n",
      "n=10 p=0.1 epoch=80 train_loss=0.21649888356526692 train_accu=0.9192833333333333 valid_loss=0.3117132097482681 valid_accu=0.8909\n",
      "n=10 p=0.1 epoch=81 train_loss=0.216617368410031 train_accu=0.9190333333333334 valid_loss=0.3130713075399399 valid_accu=0.8912\n",
      "n=10 p=0.1 epoch=82 train_loss=0.214126256108284 train_accu=0.9202833333333333 valid_loss=0.31182484030723573 valid_accu=0.8913\n",
      "n=10 p=0.1 epoch=83 train_loss=0.21172816654046375 train_accu=0.9214166666666667 valid_loss=0.3110927104949951 valid_accu=0.8926\n",
      "n=10 p=0.1 epoch=84 train_loss=0.21054241011540095 train_accu=0.92165 valid_loss=0.3111663371324539 valid_accu=0.8933\n",
      "n=10 p=0.1 epoch=85 train_loss=0.20922115221619605 train_accu=0.9224666666666667 valid_loss=0.3109098136425018 valid_accu=0.8934\n",
      "n=10 p=0.1 epoch=86 train_loss=0.20851416165630024 train_accu=0.9227166666666666 valid_loss=0.3118398666381836 valid_accu=0.8938\n",
      "n=10 p=0.1 epoch=87 train_loss=0.20671208699544272 train_accu=0.9232 valid_loss=0.31118287742137907 valid_accu=0.8933\n",
      "n=10 p=0.1 epoch=88 train_loss=0.20586979339520137 train_accu=0.9236333333333333 valid_loss=0.31139459908008577 valid_accu=0.8936\n",
      "n=10 p=0.1 epoch=89 train_loss=0.20478155066569645 train_accu=0.9239333333333334 valid_loss=0.3118509858846664 valid_accu=0.8939\n",
      "n=10 p=0.1 epoch=90 train_loss=0.2046960006157557 train_accu=0.9236833333333333 valid_loss=0.31307596564292905 valid_accu=0.8935\n",
      "n=10 p=0.1 epoch=91 train_loss=0.20517335285743077 train_accu=0.92345 valid_loss=0.3149013578891754 valid_accu=0.8931\n",
      "n=10 p=0.1 epoch=92 train_loss=0.20343513960639636 train_accu=0.9243333333333333 valid_loss=0.31422351896762846 valid_accu=0.893\n",
      "n=10 p=0.1 epoch=93 train_loss=0.2035906066497167 train_accu=0.9242 valid_loss=0.3153926253318787 valid_accu=0.8927\n",
      "n=10 p=0.1 epoch=94 train_loss=0.2030548284451167 train_accu=0.9240666666666667 valid_loss=0.3163169503211975 valid_accu=0.8916\n",
      "n=10 p=0.1 epoch=95 train_loss=0.20550530726710955 train_accu=0.92275 valid_loss=0.3197009712457657 valid_accu=0.8913\n",
      "n=10 p=0.1 epoch=96 train_loss=0.20367808664838474 train_accu=0.92355 valid_loss=0.3204526960849762 valid_accu=0.892\n",
      "n=10 p=0.1 epoch=97 train_loss=0.2042228323717912 train_accu=0.9231 valid_loss=0.3215900778770447 valid_accu=0.8922\n",
      "n=10 p=0.1 epoch=98 train_loss=0.20239600439866384 train_accu=0.92385 valid_loss=0.32121747434139253 valid_accu=0.8918\n",
      "n=10 p=0.1 epoch=99 train_loss=0.20047276889284452 train_accu=0.9247833333333333 valid_loss=0.32095709145069123 valid_accu=0.8921\n",
      "n=10 p=0.3 epoch=0 train_loss=0.9317558020353317 train_accu=0.6478 valid_loss=0.9397555410861969 valid_accu=0.6537\n",
      "n=10 p=0.3 epoch=1 train_loss=0.6695257544517517 train_accu=0.74545 valid_loss=0.6854777812957764 valid_accu=0.7443\n",
      "n=10 p=0.3 epoch=2 train_loss=0.576878896355629 train_accu=0.7729 valid_loss=0.5901031255722046 valid_accu=0.7679\n",
      "n=10 p=0.3 epoch=3 train_loss=0.5389225289225579 train_accu=0.7895 valid_loss=0.5513687789440155 valid_accu=0.7815\n",
      "n=10 p=0.3 epoch=4 train_loss=0.5118239884575208 train_accu=0.8022166666666667 valid_loss=0.5254139184951783 valid_accu=0.795\n",
      "n=10 p=0.3 epoch=5 train_loss=0.48942382285992303 train_accu=0.8126166666666667 valid_loss=0.5045301645994187 valid_accu=0.8056\n",
      "n=10 p=0.3 epoch=6 train_loss=0.4687006930510203 train_accu=0.8226666666666667 valid_loss=0.4853346228599548 valid_accu=0.8152\n",
      "n=10 p=0.3 epoch=7 train_loss=0.44916230539480845 train_accu=0.8308333333333333 valid_loss=0.46686877608299254 valid_accu=0.8238\n",
      "n=10 p=0.3 epoch=8 train_loss=0.4308218444387118 train_accu=0.8394333333333334 valid_loss=0.44930512011051177 valid_accu=0.8316\n",
      "n=10 p=0.3 epoch=9 train_loss=0.4190655911962191 train_accu=0.8463166666666667 valid_loss=0.43786144256591797 valid_accu=0.8383\n",
      "n=10 p=0.3 epoch=10 train_loss=0.4058007876078288 train_accu=0.8522666666666666 valid_loss=0.4253034442663193 valid_accu=0.8454\n",
      "n=10 p=0.3 epoch=11 train_loss=0.3960701664288839 train_accu=0.8558 valid_loss=0.4163306087255478 valid_accu=0.8499\n",
      "n=10 p=0.3 epoch=12 train_loss=0.3879817247390747 train_accu=0.85975 valid_loss=0.4088778257369995 valid_accu=0.8536\n",
      "n=10 p=0.3 epoch=13 train_loss=0.37815400809049604 train_accu=0.8636 valid_loss=0.39992069005966185 valid_accu=0.8565\n",
      "n=10 p=0.3 epoch=14 train_loss=0.37068766802549363 train_accu=0.86655 valid_loss=0.39314068853855133 valid_accu=0.8588\n",
      "n=10 p=0.3 epoch=15 train_loss=0.36082693785429 train_accu=0.8706666666666667 valid_loss=0.3838403314352036 valid_accu=0.8624\n",
      "n=10 p=0.3 epoch=16 train_loss=0.3539526849985123 train_accu=0.8723333333333333 valid_loss=0.3776791453361511 valid_accu=0.8642\n",
      "n=10 p=0.3 epoch=17 train_loss=0.3476033076643944 train_accu=0.8752166666666666 valid_loss=0.37194445431232454 valid_accu=0.8655\n",
      "n=10 p=0.3 epoch=18 train_loss=0.3401851678888003 train_accu=0.8769666666666667 valid_loss=0.3651321709156036 valid_accu=0.869\n",
      "n=10 p=0.3 epoch=19 train_loss=0.33407784352699915 train_accu=0.8794833333333333 valid_loss=0.35954258143901824 valid_accu=0.8708\n",
      "n=10 p=0.3 epoch=20 train_loss=0.3284280608097712 train_accu=0.8811833333333333 valid_loss=0.3544424891471863 valid_accu=0.8708\n",
      "n=10 p=0.3 epoch=21 train_loss=0.3211597800254822 train_accu=0.8834333333333333 valid_loss=0.34820094108581545 valid_accu=0.8734\n",
      "n=10 p=0.3 epoch=22 train_loss=0.3147161806623141 train_accu=0.8857 valid_loss=0.34247138500213625 valid_accu=0.8767\n",
      "n=10 p=0.3 epoch=23 train_loss=0.30935647735993066 train_accu=0.8873833333333333 valid_loss=0.33845093846321106 valid_accu=0.8787\n",
      "n=10 p=0.3 epoch=24 train_loss=0.30373622924089433 train_accu=0.8892166666666667 valid_loss=0.3336654305458069 valid_accu=0.8798\n",
      "n=10 p=0.3 epoch=25 train_loss=0.29977240214745204 train_accu=0.8902833333333333 valid_loss=0.3311057299375534 valid_accu=0.8798\n",
      "n=10 p=0.3 epoch=26 train_loss=0.29523878594239555 train_accu=0.89165 valid_loss=0.32790004909038545 valid_accu=0.8816\n",
      "n=10 p=0.3 epoch=27 train_loss=0.2912199189265569 train_accu=0.8936 valid_loss=0.32494647800922394 valid_accu=0.8818\n",
      "n=10 p=0.3 epoch=28 train_loss=0.28843795831004776 train_accu=0.8941666666666667 valid_loss=0.3233748733997345 valid_accu=0.8826\n",
      "n=10 p=0.3 epoch=29 train_loss=0.28592047095298767 train_accu=0.8950666666666667 valid_loss=0.3219975769519806 valid_accu=0.8831\n",
      "n=10 p=0.3 epoch=30 train_loss=0.2835689147313436 train_accu=0.8954333333333333 valid_loss=0.3207843154668808 valid_accu=0.8839\n",
      "n=10 p=0.3 epoch=31 train_loss=0.2801202322045962 train_accu=0.8971166666666667 valid_loss=0.318253293633461 valid_accu=0.885\n",
      "n=10 p=0.3 epoch=32 train_loss=0.27805929705500604 train_accu=0.8977166666666667 valid_loss=0.31721738278865813 valid_accu=0.8855\n",
      "n=10 p=0.3 epoch=33 train_loss=0.27519559587041537 train_accu=0.8985833333333333 valid_loss=0.3152649223804474 valid_accu=0.8855\n",
      "n=10 p=0.3 epoch=34 train_loss=0.2731564079721769 train_accu=0.8994333333333333 valid_loss=0.31423182785511017 valid_accu=0.8861\n",
      "n=10 p=0.3 epoch=35 train_loss=0.27052450850605964 train_accu=0.90045 valid_loss=0.3124189615249634 valid_accu=0.8863\n",
      "n=10 p=0.3 epoch=36 train_loss=0.26907464290658634 train_accu=0.90075 valid_loss=0.3121541619300842 valid_accu=0.8873\n",
      "n=10 p=0.3 epoch=37 train_loss=0.2666824874778589 train_accu=0.9016166666666666 valid_loss=0.3106047362089157 valid_accu=0.8882\n",
      "n=10 p=0.3 epoch=38 train_loss=0.2641400136053562 train_accu=0.9025666666666666 valid_loss=0.3095914930105209 valid_accu=0.8875\n",
      "n=10 p=0.3 epoch=39 train_loss=0.2616437889635563 train_accu=0.90295 valid_loss=0.30818056762218476 valid_accu=0.8879\n",
      "n=10 p=0.3 epoch=40 train_loss=0.2596290682752927 train_accu=0.9038666666666667 valid_loss=0.3072065234184265 valid_accu=0.8883\n",
      "n=10 p=0.3 epoch=41 train_loss=0.2569650282462438 train_accu=0.9050166666666667 valid_loss=0.30551124811172486 valid_accu=0.8889\n",
      "n=10 p=0.3 epoch=42 train_loss=0.25477351496617 train_accu=0.9056666666666666 valid_loss=0.30459223687648773 valid_accu=0.8892\n",
      "n=10 p=0.3 epoch=43 train_loss=0.25282302126288414 train_accu=0.9066 valid_loss=0.3038370043039322 valid_accu=0.89\n",
      "n=10 p=0.3 epoch=44 train_loss=0.25023708219329516 train_accu=0.9077 valid_loss=0.30161321461200713 valid_accu=0.8911\n",
      "n=10 p=0.3 epoch=45 train_loss=0.24874515036741893 train_accu=0.9083833333333333 valid_loss=0.30114705860614777 valid_accu=0.8914\n",
      "n=10 p=0.3 epoch=46 train_loss=0.24626970589160918 train_accu=0.9092166666666667 valid_loss=0.29948593974113463 valid_accu=0.892\n",
      "n=10 p=0.3 epoch=47 train_loss=0.24302736123402913 train_accu=0.9101833333333333 valid_loss=0.29699622094631195 valid_accu=0.8927\n",
      "n=10 p=0.3 epoch=48 train_loss=0.24112081974744798 train_accu=0.9107666666666666 valid_loss=0.29624669849872587 valid_accu=0.8928\n",
      "n=10 p=0.3 epoch=49 train_loss=0.2390237492819627 train_accu=0.9116 valid_loss=0.2952552795410156 valid_accu=0.893\n",
      "n=10 p=0.3 epoch=50 train_loss=0.23747307633360226 train_accu=0.9121 valid_loss=0.29498857259750366 valid_accu=0.8926\n",
      "n=10 p=0.3 epoch=51 train_loss=0.23608641053239504 train_accu=0.9124666666666666 valid_loss=0.29437500089406965 valid_accu=0.8929\n",
      "n=10 p=0.3 epoch=52 train_loss=0.23414835159977276 train_accu=0.91345 valid_loss=0.29354040175676344 valid_accu=0.8931\n",
      "n=10 p=0.3 epoch=53 train_loss=0.2322305289407571 train_accu=0.9138666666666667 valid_loss=0.2926915526390076 valid_accu=0.8932\n",
      "n=10 p=0.3 epoch=54 train_loss=0.23104962855577468 train_accu=0.9144833333333333 valid_loss=0.2925615251064301 valid_accu=0.8938\n",
      "n=10 p=0.3 epoch=55 train_loss=0.22926806112130482 train_accu=0.9149666666666667 valid_loss=0.2921894133090973 valid_accu=0.8945\n",
      "n=10 p=0.3 epoch=56 train_loss=0.2281989149749279 train_accu=0.9149333333333334 valid_loss=0.29230725318193435 valid_accu=0.8948\n",
      "n=10 p=0.3 epoch=57 train_loss=0.22618993520736694 train_accu=0.9160833333333334 valid_loss=0.29139872193336486 valid_accu=0.8949\n",
      "n=10 p=0.3 epoch=58 train_loss=0.22507844840486843 train_accu=0.9159166666666667 valid_loss=0.29172581881284715 valid_accu=0.895\n",
      "n=10 p=0.3 epoch=59 train_loss=0.22373474687337874 train_accu=0.9165333333333333 valid_loss=0.2911140099167824 valid_accu=0.8951\n",
      "n=10 p=0.3 epoch=60 train_loss=0.22264466136693956 train_accu=0.9169666666666667 valid_loss=0.2911773607134819 valid_accu=0.8953\n",
      "n=10 p=0.3 epoch=61 train_loss=0.22184855143229168 train_accu=0.9174833333333333 valid_loss=0.29126298278570173 valid_accu=0.8946\n",
      "n=10 p=0.3 epoch=62 train_loss=0.22197470168272654 train_accu=0.9170166666666667 valid_loss=0.2920390903949738 valid_accu=0.8936\n",
      "n=10 p=0.3 epoch=63 train_loss=0.2230656050145626 train_accu=0.9164666666666667 valid_loss=0.2942143127322197 valid_accu=0.893\n",
      "n=10 p=0.3 epoch=64 train_loss=0.2223748691380024 train_accu=0.9168 valid_loss=0.29489766508340837 valid_accu=0.8927\n",
      "n=10 p=0.3 epoch=65 train_loss=0.22270796075463295 train_accu=0.91675 valid_loss=0.29620765894651413 valid_accu=0.8927\n",
      "n=10 p=0.3 epoch=66 train_loss=0.22299615566929182 train_accu=0.9169333333333334 valid_loss=0.29711131900548937 valid_accu=0.8932\n",
      "n=10 p=0.3 epoch=67 train_loss=0.22509283249576886 train_accu=0.9157333333333333 valid_loss=0.30011168867349625 valid_accu=0.8921\n",
      "n=10 p=0.3 epoch=68 train_loss=0.22774479414025942 train_accu=0.9143 valid_loss=0.30315113365650176 valid_accu=0.8914\n",
      "n=10 p=0.3 epoch=69 train_loss=0.22962630117932956 train_accu=0.9137666666666666 valid_loss=0.3045061081647873 valid_accu=0.8912\n",
      "n=10 p=0.3 epoch=70 train_loss=0.22479182357589403 train_accu=0.9154 valid_loss=0.3014985591173172 valid_accu=0.8913\n",
      "n=10 p=0.3 epoch=71 train_loss=0.22002650648355485 train_accu=0.9174833333333333 valid_loss=0.2986977010965347 valid_accu=0.8932\n",
      "n=10 p=0.3 epoch=72 train_loss=0.21834079772233964 train_accu=0.9182166666666667 valid_loss=0.2986392959952354 valid_accu=0.8938\n",
      "n=10 p=0.3 epoch=73 train_loss=0.21589549953738849 train_accu=0.91915 valid_loss=0.29797756373882295 valid_accu=0.8941\n",
      "n=10 p=0.3 epoch=74 train_loss=0.21559976264834405 train_accu=0.9192 valid_loss=0.2988976612687111 valid_accu=0.8949\n",
      "n=10 p=0.3 epoch=75 train_loss=0.21547073523203533 train_accu=0.9193166666666667 valid_loss=0.30052700638771057 valid_accu=0.8936\n",
      "n=10 p=0.3 epoch=76 train_loss=0.2160438321530819 train_accu=0.9189666666666667 valid_loss=0.30278133451938627 valid_accu=0.8941\n",
      "n=10 p=0.3 epoch=77 train_loss=0.21623416220148403 train_accu=0.9186666666666666 valid_loss=0.3044200301170349 valid_accu=0.8938\n",
      "n=10 p=0.3 epoch=78 train_loss=0.21537645533680916 train_accu=0.91905 valid_loss=0.3047089636325836 valid_accu=0.8941\n",
      "n=10 p=0.3 epoch=79 train_loss=0.21411780665318172 train_accu=0.9194666666666667 valid_loss=0.30472848415374754 valid_accu=0.8947\n",
      "n=10 p=0.3 epoch=80 train_loss=0.21129361689090728 train_accu=0.9206666666666666 valid_loss=0.3031453862786293 valid_accu=0.895\n",
      "n=10 p=0.3 epoch=81 train_loss=0.20940423980355263 train_accu=0.9215166666666667 valid_loss=0.30265903323888776 valid_accu=0.8959\n",
      "n=10 p=0.3 epoch=82 train_loss=0.205618101110061 train_accu=0.9230333333333334 valid_loss=0.29974267184734343 valid_accu=0.8967\n",
      "n=10 p=0.3 epoch=83 train_loss=0.20186453610658645 train_accu=0.9246333333333333 valid_loss=0.29707548320293425 valid_accu=0.8971\n",
      "n=10 p=0.3 epoch=84 train_loss=0.19846321046352386 train_accu=0.9259 valid_loss=0.2942056953907013 valid_accu=0.8984\n",
      "n=10 p=0.3 epoch=85 train_loss=0.19534002393484115 train_accu=0.9272666666666667 valid_loss=0.2923765167593956 valid_accu=0.8987\n",
      "n=10 p=0.3 epoch=86 train_loss=0.19356038322051367 train_accu=0.9278166666666666 valid_loss=0.2916121006011963 valid_accu=0.899\n",
      "n=10 p=0.3 epoch=87 train_loss=0.19227139229575793 train_accu=0.92815 valid_loss=0.29172560274600984 valid_accu=0.8991\n",
      "n=10 p=0.3 epoch=88 train_loss=0.1910104990005493 train_accu=0.9283166666666667 valid_loss=0.2919900432229042 valid_accu=0.8991\n",
      "n=10 p=0.3 epoch=89 train_loss=0.18969149589538575 train_accu=0.92925 valid_loss=0.2918348640203476 valid_accu=0.8977\n",
      "n=10 p=0.3 epoch=90 train_loss=0.1885162663956483 train_accu=0.9296833333333333 valid_loss=0.291786701977253 valid_accu=0.8986\n",
      "n=10 p=0.3 epoch=91 train_loss=0.18659651925166448 train_accu=0.93045 valid_loss=0.2911390051245689 valid_accu=0.8989\n",
      "n=10 p=0.3 epoch=92 train_loss=0.18583284045259157 train_accu=0.9308666666666666 valid_loss=0.2914370819926262 valid_accu=0.8988\n",
      "n=10 p=0.3 epoch=93 train_loss=0.18493423039714496 train_accu=0.9312666666666667 valid_loss=0.2915733367204666 valid_accu=0.8995\n",
      "n=10 p=0.3 epoch=94 train_loss=0.18368401601910592 train_accu=0.9314166666666667 valid_loss=0.2914357990026474 valid_accu=0.8988\n",
      "n=10 p=0.3 epoch=95 train_loss=0.18241434320807456 train_accu=0.9321333333333334 valid_loss=0.2915287375450134 valid_accu=0.8993\n",
      "n=10 p=0.3 epoch=96 train_loss=0.1816332831978798 train_accu=0.93235 valid_loss=0.29187595993280413 valid_accu=0.8999\n",
      "n=10 p=0.3 epoch=97 train_loss=0.18109015598893166 train_accu=0.9324166666666667 valid_loss=0.2925284132361412 valid_accu=0.9\n",
      "n=10 p=0.3 epoch=98 train_loss=0.17997415785988172 train_accu=0.93285 valid_loss=0.29333910197019575 valid_accu=0.9002\n",
      "n=10 p=0.3 epoch=99 train_loss=0.179928773889939 train_accu=0.9327333333333333 valid_loss=0.294735349714756 valid_accu=0.8989\n",
      "n=10 p=0.5 epoch=0 train_loss=0.9037960112094879 train_accu=0.6700166666666667 valid_loss=0.9193741142749786 valid_accu=0.6642\n",
      "n=10 p=0.5 epoch=1 train_loss=0.6347238649924596 train_accu=0.7517166666666667 valid_loss=0.6535760462284088 valid_accu=0.7475\n",
      "n=10 p=0.5 epoch=2 train_loss=0.5662244454026222 train_accu=0.7798333333333334 valid_loss=0.5855005502700805 valid_accu=0.7736\n",
      "n=10 p=0.5 epoch=3 train_loss=0.5218755270044009 train_accu=0.7988333333333333 valid_loss=0.5427657902240753 valid_accu=0.7871\n",
      "n=10 p=0.5 epoch=4 train_loss=0.49221962243318557 train_accu=0.8126166666666667 valid_loss=0.5142731755971909 valid_accu=0.8018\n",
      "n=10 p=0.5 epoch=5 train_loss=0.46701172441244126 train_accu=0.8255333333333333 valid_loss=0.49037529826164244 valid_accu=0.8174\n",
      "n=10 p=0.5 epoch=6 train_loss=0.44409780303637186 train_accu=0.8371 valid_loss=0.46987133026123046 valid_accu=0.8291\n",
      "n=10 p=0.5 epoch=7 train_loss=0.42230744908253354 train_accu=0.8467666666666667 valid_loss=0.45046300888061525 valid_accu=0.8376\n",
      "n=10 p=0.5 epoch=8 train_loss=0.40472105940183006 train_accu=0.8526 valid_loss=0.4340185075998306 valid_accu=0.8427\n",
      "n=10 p=0.5 epoch=9 train_loss=0.3897588019569715 train_accu=0.8572666666666666 valid_loss=0.41965135633945466 valid_accu=0.8496\n",
      "n=10 p=0.5 epoch=10 train_loss=0.38024042596419655 train_accu=0.8609 valid_loss=0.41081000566482545 valid_accu=0.8513\n",
      "n=10 p=0.5 epoch=11 train_loss=0.37186821003754933 train_accu=0.8648 valid_loss=0.40283083617687226 valid_accu=0.8536\n",
      "n=10 p=0.5 epoch=12 train_loss=0.36878503759702047 train_accu=0.8654666666666667 valid_loss=0.4001620262861252 valid_accu=0.854\n",
      "n=10 p=0.5 epoch=13 train_loss=0.3604270339012146 train_accu=0.8684 valid_loss=0.39283819794654845 valid_accu=0.8563\n",
      "n=10 p=0.5 epoch=14 train_loss=0.35169278184572855 train_accu=0.8714333333333333 valid_loss=0.38537726998329164 valid_accu=0.8591\n",
      "n=10 p=0.5 epoch=15 train_loss=0.34646068612734476 train_accu=0.8727 valid_loss=0.3809745103120804 valid_accu=0.8612\n",
      "n=10 p=0.5 epoch=16 train_loss=0.3417294705907504 train_accu=0.8747166666666667 valid_loss=0.37702334523200987 valid_accu=0.8622\n",
      "n=10 p=0.5 epoch=17 train_loss=0.3355876748760541 train_accu=0.8765666666666667 valid_loss=0.37179107666015626 valid_accu=0.8653\n",
      "n=10 p=0.5 epoch=18 train_loss=0.33097543915112815 train_accu=0.8782666666666666 valid_loss=0.36799130141735076 valid_accu=0.8664\n",
      "n=10 p=0.5 epoch=19 train_loss=0.3268214796980222 train_accu=0.8805333333333333 valid_loss=0.3644916981458664 valid_accu=0.8674\n",
      "n=10 p=0.5 epoch=20 train_loss=0.3217635581890742 train_accu=0.8820166666666667 valid_loss=0.3606244444847107 valid_accu=0.8689\n",
      "n=10 p=0.5 epoch=21 train_loss=0.3174527222911517 train_accu=0.8832833333333333 valid_loss=0.35727608799934385 valid_accu=0.8696\n",
      "n=10 p=0.5 epoch=22 train_loss=0.31177317996819814 train_accu=0.8851833333333333 valid_loss=0.35279822945594785 valid_accu=0.8706\n",
      "n=10 p=0.5 epoch=23 train_loss=0.308141886194547 train_accu=0.88605 valid_loss=0.35031055212020873 valid_accu=0.8704\n",
      "n=10 p=0.5 epoch=24 train_loss=0.30495480448007584 train_accu=0.8874166666666666 valid_loss=0.3475273162126541 valid_accu=0.8713\n",
      "n=10 p=0.5 epoch=25 train_loss=0.3012640962998072 train_accu=0.8888666666666667 valid_loss=0.34482012689113617 valid_accu=0.8721\n",
      "n=10 p=0.5 epoch=26 train_loss=0.29875495557983717 train_accu=0.8898166666666667 valid_loss=0.3433427602052689 valid_accu=0.8733\n",
      "n=10 p=0.5 epoch=27 train_loss=0.29588301926851274 train_accu=0.8908 valid_loss=0.34147284626960756 valid_accu=0.8739\n",
      "n=10 p=0.5 epoch=28 train_loss=0.2920212236543496 train_accu=0.89225 valid_loss=0.3386538654565811 valid_accu=0.8744\n",
      "n=10 p=0.5 epoch=29 train_loss=0.2908078027268251 train_accu=0.8928333333333334 valid_loss=0.33807505667209625 valid_accu=0.8737\n",
      "n=10 p=0.5 epoch=30 train_loss=0.28597456564505896 train_accu=0.8946333333333333 valid_loss=0.3348184108734131 valid_accu=0.8758\n",
      "n=10 p=0.5 epoch=31 train_loss=0.28265763595700266 train_accu=0.8959333333333334 valid_loss=0.3324033707380295 valid_accu=0.8768\n",
      "n=10 p=0.5 epoch=32 train_loss=0.2794594205915928 train_accu=0.8970666666666667 valid_loss=0.3305862724781036 valid_accu=0.8785\n",
      "n=10 p=0.5 epoch=33 train_loss=0.27698687463998795 train_accu=0.8975833333333333 valid_loss=0.3289389282464981 valid_accu=0.8805\n",
      "n=10 p=0.5 epoch=34 train_loss=0.2747878387570381 train_accu=0.8983833333333333 valid_loss=0.32742977142333984 valid_accu=0.8806\n",
      "n=10 p=0.5 epoch=35 train_loss=0.27276485015948615 train_accu=0.8994666666666666 valid_loss=0.3267095983028412 valid_accu=0.882\n",
      "n=10 p=0.5 epoch=36 train_loss=0.2700584637622038 train_accu=0.9003 valid_loss=0.32469230592250825 valid_accu=0.8828\n",
      "n=10 p=0.5 epoch=37 train_loss=0.266792360941569 train_accu=0.9014333333333333 valid_loss=0.32318198382854463 valid_accu=0.8842\n",
      "n=10 p=0.5 epoch=38 train_loss=0.266449186950922 train_accu=0.9012666666666667 valid_loss=0.3235029339790344 valid_accu=0.8838\n",
      "n=10 p=0.5 epoch=39 train_loss=0.2627931497991085 train_accu=0.9032 valid_loss=0.32112212777137755 valid_accu=0.8842\n",
      "n=10 p=0.5 epoch=40 train_loss=0.2609670557081699 train_accu=0.9036666666666666 valid_loss=0.3204174339771271 valid_accu=0.8852\n",
      "n=10 p=0.5 epoch=41 train_loss=0.2603075481951237 train_accu=0.9042 valid_loss=0.32043416798114777 valid_accu=0.8858\n",
      "n=10 p=0.5 epoch=42 train_loss=0.25832281981905303 train_accu=0.9046833333333333 valid_loss=0.3200922906398773 valid_accu=0.8859\n",
      "n=10 p=0.5 epoch=43 train_loss=0.25932265520095826 train_accu=0.9042333333333333 valid_loss=0.32216515839099885 valid_accu=0.8858\n",
      "n=10 p=0.5 epoch=44 train_loss=0.258906531582276 train_accu=0.90425 valid_loss=0.3227176457643509 valid_accu=0.886\n",
      "n=10 p=0.5 epoch=45 train_loss=0.25679745972156526 train_accu=0.9051 valid_loss=0.32245166003704073 valid_accu=0.8856\n",
      "n=10 p=0.5 epoch=46 train_loss=0.2548448900381724 train_accu=0.9059833333333334 valid_loss=0.3219775855541229 valid_accu=0.8861\n",
      "n=10 p=0.5 epoch=47 train_loss=0.25261326258381206 train_accu=0.9066666666666666 valid_loss=0.3204902410507202 valid_accu=0.8862\n",
      "n=10 p=0.5 epoch=48 train_loss=0.25012969026962917 train_accu=0.9079 valid_loss=0.31872382164001467 valid_accu=0.8878\n",
      "n=10 p=0.5 epoch=49 train_loss=0.2479449465870857 train_accu=0.9086833333333333 valid_loss=0.31763256788253785 valid_accu=0.8876\n",
      "n=10 p=0.5 epoch=50 train_loss=0.24567368850111962 train_accu=0.9096666666666666 valid_loss=0.3162087261676788 valid_accu=0.8882\n",
      "n=10 p=0.5 epoch=51 train_loss=0.2420672299961249 train_accu=0.9109333333333334 valid_loss=0.3149199038743973 valid_accu=0.8897\n",
      "n=10 p=0.5 epoch=52 train_loss=0.24057224864761034 train_accu=0.91155 valid_loss=0.3143021374940872 valid_accu=0.8895\n",
      "n=10 p=0.5 epoch=53 train_loss=0.23684391006827354 train_accu=0.9124333333333333 valid_loss=0.3117911994457245 valid_accu=0.8916\n",
      "n=10 p=0.5 epoch=54 train_loss=0.2341474821170171 train_accu=0.9139166666666667 valid_loss=0.31046950817108154 valid_accu=0.8915\n",
      "n=10 p=0.5 epoch=55 train_loss=0.23268939356009166 train_accu=0.9139 valid_loss=0.30976209342479705 valid_accu=0.8922\n",
      "n=10 p=0.5 epoch=56 train_loss=0.23041473825772604 train_accu=0.9148166666666666 valid_loss=0.3085847467184067 valid_accu=0.8929\n",
      "n=10 p=0.5 epoch=57 train_loss=0.2300539786616961 train_accu=0.9148333333333334 valid_loss=0.31002486050128936 valid_accu=0.8917\n",
      "n=10 p=0.5 epoch=58 train_loss=0.22972418318192164 train_accu=0.9149666666666667 valid_loss=0.3111771285533905 valid_accu=0.8916\n",
      "n=10 p=0.5 epoch=59 train_loss=0.22590826054414112 train_accu=0.9168166666666666 valid_loss=0.30844093561172486 valid_accu=0.8916\n",
      "n=10 p=0.5 epoch=60 train_loss=0.22317626749475797 train_accu=0.9174166666666667 valid_loss=0.30784650444984435 valid_accu=0.8927\n",
      "n=10 p=0.5 epoch=61 train_loss=0.22237454652786254 train_accu=0.91755 valid_loss=0.30900387167930604 valid_accu=0.8927\n",
      "n=10 p=0.5 epoch=62 train_loss=0.22012433608373005 train_accu=0.9182166666666667 valid_loss=0.3084944665431976 valid_accu=0.8928\n",
      "n=10 p=0.5 epoch=63 train_loss=0.21886265948414801 train_accu=0.9183 valid_loss=0.3085988312959671 valid_accu=0.893\n",
      "n=10 p=0.5 epoch=64 train_loss=0.21731255700190863 train_accu=0.9189666666666667 valid_loss=0.3085085391998291 valid_accu=0.8937\n",
      "n=10 p=0.5 epoch=65 train_loss=0.2151598813633124 train_accu=0.9197833333333333 valid_loss=0.308509436249733 valid_accu=0.8926\n",
      "n=10 p=0.5 epoch=66 train_loss=0.21297391007343927 train_accu=0.92035 valid_loss=0.30783564448356626 valid_accu=0.8934\n",
      "n=10 p=0.5 epoch=67 train_loss=0.21207860484719276 train_accu=0.9206666666666666 valid_loss=0.30835707783699035 valid_accu=0.8938\n",
      "n=10 p=0.5 epoch=68 train_loss=0.21047101095318793 train_accu=0.9208 valid_loss=0.3090436488389969 valid_accu=0.8948\n",
      "n=10 p=0.5 epoch=69 train_loss=0.20987799912691116 train_accu=0.92095 valid_loss=0.30980224907398224 valid_accu=0.8946\n",
      "n=10 p=0.5 epoch=70 train_loss=0.20845816358923913 train_accu=0.9217666666666666 valid_loss=0.3097985714673996 valid_accu=0.8944\n",
      "n=10 p=0.5 epoch=71 train_loss=0.20625266854961713 train_accu=0.9224666666666667 valid_loss=0.3095185995101929 valid_accu=0.8947\n",
      "n=10 p=0.5 epoch=72 train_loss=0.20516904890537263 train_accu=0.9231666666666667 valid_loss=0.3105781555175781 valid_accu=0.8951\n",
      "n=10 p=0.5 epoch=73 train_loss=0.20498207435011864 train_accu=0.9229166666666667 valid_loss=0.31170949935913084 valid_accu=0.8948\n",
      "n=10 p=0.5 epoch=74 train_loss=0.2045763892432054 train_accu=0.9231833333333334 valid_loss=0.31304377913475034 valid_accu=0.8946\n",
      "n=10 p=0.5 epoch=75 train_loss=0.20272578274210293 train_accu=0.9241 valid_loss=0.31293036937713625 valid_accu=0.8954\n",
      "n=10 p=0.5 epoch=76 train_loss=0.20191727181275684 train_accu=0.9242666666666667 valid_loss=0.31360967457294464 valid_accu=0.896\n",
      "n=10 p=0.5 epoch=77 train_loss=0.2012838969628016 train_accu=0.9249 valid_loss=0.3147218555212021 valid_accu=0.8958\n",
      "n=10 p=0.5 epoch=78 train_loss=0.2021656033893426 train_accu=0.9242666666666667 valid_loss=0.31616970598697663 valid_accu=0.8958\n",
      "n=10 p=0.5 epoch=79 train_loss=0.2050211285551389 train_accu=0.9231833333333334 valid_loss=0.3194951891899109 valid_accu=0.894\n",
      "n=10 p=0.5 epoch=80 train_loss=0.21682212700446446 train_accu=0.9175 valid_loss=0.33380050361156466 valid_accu=0.8897\n",
      "n=10 p=0.5 epoch=81 train_loss=0.20569834808508555 train_accu=0.9219833333333334 valid_loss=0.32441904544830324 valid_accu=0.8903\n",
      "n=10 p=0.5 epoch=82 train_loss=0.19921741237243015 train_accu=0.9244166666666667 valid_loss=0.31946862041950225 valid_accu=0.8919\n",
      "n=10 p=0.5 epoch=83 train_loss=0.1937225192785263 train_accu=0.9268166666666666 valid_loss=0.3147104293107986 valid_accu=0.8936\n",
      "n=10 p=0.5 epoch=84 train_loss=0.19007349386811256 train_accu=0.9294166666666667 valid_loss=0.311917382478714 valid_accu=0.896\n",
      "n=10 p=0.5 epoch=85 train_loss=0.18887238577008247 train_accu=0.9297666666666666 valid_loss=0.3133204966783524 valid_accu=0.8965\n",
      "n=10 p=0.5 epoch=86 train_loss=0.18745558559894562 train_accu=0.93055 valid_loss=0.3135540336370468 valid_accu=0.8969\n",
      "n=10 p=0.5 epoch=87 train_loss=0.18921248689293862 train_accu=0.9296333333333333 valid_loss=0.31823436319828036 valid_accu=0.8956\n",
      "n=10 p=0.5 epoch=88 train_loss=0.18944399580359458 train_accu=0.9295333333333333 valid_loss=0.32067201733589173 valid_accu=0.8937\n",
      "n=10 p=0.5 epoch=89 train_loss=0.18869327157735824 train_accu=0.92945 valid_loss=0.321466463804245 valid_accu=0.8943\n",
      "n=10 p=0.5 epoch=90 train_loss=0.18847949380675952 train_accu=0.9295666666666667 valid_loss=0.32254748344421386 valid_accu=0.8931\n",
      "n=10 p=0.5 epoch=91 train_loss=0.1866286853949229 train_accu=0.9310666666666667 valid_loss=0.3220910757780075 valid_accu=0.8941\n",
      "n=10 p=0.5 epoch=92 train_loss=0.18570161486665407 train_accu=0.93105 valid_loss=0.32252492010593414 valid_accu=0.895\n",
      "n=10 p=0.5 epoch=93 train_loss=0.18401237179835636 train_accu=0.9312333333333334 valid_loss=0.3222189247608185 valid_accu=0.8942\n",
      "n=10 p=0.5 epoch=94 train_loss=0.18229222272833187 train_accu=0.9316833333333333 valid_loss=0.32320294976234437 valid_accu=0.8949\n",
      "n=10 p=0.5 epoch=95 train_loss=0.181258108963569 train_accu=0.9323333333333333 valid_loss=0.32342836558818816 valid_accu=0.8945\n",
      "n=10 p=0.5 epoch=96 train_loss=0.1839120420316855 train_accu=0.9310666666666667 valid_loss=0.3274319440126419 valid_accu=0.8939\n",
      "n=10 p=0.5 epoch=97 train_loss=0.1823194111386935 train_accu=0.93175 valid_loss=0.3281227171421051 valid_accu=0.8941\n",
      "n=10 p=0.5 epoch=98 train_loss=0.18118900309006372 train_accu=0.9324333333333333 valid_loss=0.3289486557245255 valid_accu=0.8938\n",
      "n=10 p=0.5 epoch=99 train_loss=0.1793084477384885 train_accu=0.9333333333333333 valid_loss=0.3285925298929214 valid_accu=0.8937\n"
     ]
    }
   ],
   "source": [
    "# Definimos hiperparámetros de entrenamiento\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1000\n",
    "num_epochs = 100\n",
    "num_k = 1 #72\n",
    "n=10 # Recordar que 28*28=784\n",
    "dropouts=[0.1,0.3,0.5]\n",
    "# Creamos una funcion de perdida\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Creamos un DataFrame de pandas para ir almacenando los valores calculados.\n",
    "df = pd.DataFrame()\n",
    "# Simulamos por tramos porque google colab se desconecta antes de que concluya para todos los valores de n en la lista.\n",
    "for p in dropouts:\n",
    "#for k in range(num_k):\n",
    "    # Creamos el modelo y el optimzador\n",
    "    model = Network(p)\n",
    "    # Creamos los dataloaders ...\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=batch_size)\n",
    "    # ... en particular, usamos el dataset de prueba (test) como dataset de validación\n",
    "    valid_dataloader = DataLoader(test_dataset,batch_size=batch_size)         \n",
    "    #optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "    # Entrenamos el modelo y calcualmos curvas.\n",
    "    min_valid_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loop(train_dataloader,model,loss_fn,optimizer)\n",
    "        train_loss,train_accu = test_loop(train_dataloader,model,loss_fn)\n",
    "        valid_loss,valid_accu = test_loop(valid_dataloader,model,loss_fn)\n",
    "        print(f\"n={n} p={p} epoch={epoch} train_loss={train_loss} train_accu={train_accu} valid_loss={valid_loss} valid_accu={valid_accu}\")\n",
    "        df = df.append({\"n\":n,\n",
    "                        \"p\":p,\n",
    "                        \"epoch\":epoch,\n",
    "                        \"train_loss\":train_loss,\n",
    "                        \"train_accu\":train_accu,\n",
    "                        \"valid_loss\":valid_loss,\n",
    "                        \"valid_accu\":valid_accu}\n",
    "                        ,ignore_index=True)\n",
    "json_fname = \"simulation-results-\"+datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")+\".json\"\n",
    "df.to_json(json_fname)\n",
    "if COLAB:\n",
    "    files.download(json_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          n    k  epoch  train_loss  train_accu  valid_loss  valid_accu    p\n",
      "0    2048.0  0.0      0    0.324914    0.879900    0.358151      0.8712  NaN\n",
      "1    2048.0  0.0      1    0.318141    0.882533    0.351912      0.8740  NaN\n",
      "2    2048.0  0.0      2    0.312732    0.884967    0.347244      0.8755  NaN\n",
      "3    2048.0  0.0      3    0.307159    0.886817    0.343223      0.8769  NaN\n",
      "4    2048.0  0.0      4    0.302259    0.888633    0.339378      0.8781  NaN\n",
      "..      ...  ...    ...         ...         ...         ...         ...  ...\n",
      "469    10.0  NaN     95    0.181258    0.932333    0.323428      0.8945  0.5\n",
      "470    10.0  NaN     96    0.183912    0.931067    0.327432      0.8939  0.5\n",
      "471    10.0  NaN     97    0.182319    0.931750    0.328123      0.8941  0.5\n",
      "472    10.0  NaN     98    0.181189    0.932433    0.328949      0.8938  0.5\n",
      "473    10.0  NaN     99    0.179308    0.933333    0.328593      0.8937  0.5\n",
      "\n",
      "[474 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 474 entries, 0 to 473\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   n           425 non-null    float64\n",
      " 1   k           174 non-null    float64\n",
      " 2   epoch       474 non-null    int64  \n",
      " 3   train_loss  474 non-null    float64\n",
      " 4   train_accu  474 non-null    float64\n",
      " 5   valid_loss  474 non-null    float64\n",
      " 6   valid_accu  474 non-null    float64\n",
      " 7   p           300 non-null    float64\n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 29.8 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out list_json\n",
    "# Usamos el bash magic de Jupyter para ver que archivos *.json hemos creado.\n",
    "# Guardamos el resultado en la variable list_json\n",
    "ls *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simulation-results-2022-02-20-01-13-58.json',\n",
       " 'simulation-results-2022-02-20-01-15-42.json',\n",
       " 'simulation-results-2022-02-20-01-22-58.json',\n",
       " 'simulation-results-2022-02-20-01-29-25.json',\n",
       " 'simulation-results-2022-02-20-01-46-31.json',\n",
       " 'simulation-results-2022-02-20-13-26-42.json',\n",
       " 'simulation-results-2022-02-20-14-30-21.json',\n",
       " 'simulation-results-2022-02-20-14-39-32.json',\n",
       " 'simulation-results-2022-02-20-15-05-27.json',\n",
       " 'simulation-results-2022-02-20-15-18-25.json',\n",
       " 'simulation-results-2022-02-20-15-25-11.json',\n",
       " 'simulation-results-2022-02-20-16-10-09.json',\n",
       " 'simulation-results-2022-02-20-18-06-20.json']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_json = list_json.split()\n",
    "list_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>k</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accu</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accu</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.324914</td>\n",
       "      <td>0.879900</td>\n",
       "      <td>0.358151</td>\n",
       "      <td>0.8712</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.318141</td>\n",
       "      <td>0.882533</td>\n",
       "      <td>0.351912</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.312732</td>\n",
       "      <td>0.884967</td>\n",
       "      <td>0.347244</td>\n",
       "      <td>0.8755</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.307159</td>\n",
       "      <td>0.886817</td>\n",
       "      <td>0.343223</td>\n",
       "      <td>0.8769</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.302259</td>\n",
       "      <td>0.888633</td>\n",
       "      <td>0.339378</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95</td>\n",
       "      <td>0.181258</td>\n",
       "      <td>0.932333</td>\n",
       "      <td>0.323428</td>\n",
       "      <td>0.8945</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>0.183912</td>\n",
       "      <td>0.931067</td>\n",
       "      <td>0.327432</td>\n",
       "      <td>0.8939</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97</td>\n",
       "      <td>0.182319</td>\n",
       "      <td>0.931750</td>\n",
       "      <td>0.328123</td>\n",
       "      <td>0.8941</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>0.181189</td>\n",
       "      <td>0.932433</td>\n",
       "      <td>0.328949</td>\n",
       "      <td>0.8938</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.179308</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.328593</td>\n",
       "      <td>0.8937</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>474 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          n    k  epoch  train_loss  train_accu  valid_loss  valid_accu    p\n",
       "0    2048.0  0.0      0    0.324914    0.879900    0.358151      0.8712  NaN\n",
       "1    2048.0  0.0      1    0.318141    0.882533    0.351912      0.8740  NaN\n",
       "2    2048.0  0.0      2    0.312732    0.884967    0.347244      0.8755  NaN\n",
       "3    2048.0  0.0      3    0.307159    0.886817    0.343223      0.8769  NaN\n",
       "4    2048.0  0.0      4    0.302259    0.888633    0.339378      0.8781  NaN\n",
       "..      ...  ...    ...         ...         ...         ...         ...  ...\n",
       "469    10.0  NaN     95    0.181258    0.932333    0.323428      0.8945  0.5\n",
       "470    10.0  NaN     96    0.183912    0.931067    0.327432      0.8939  0.5\n",
       "471    10.0  NaN     97    0.182319    0.931750    0.328123      0.8941  0.5\n",
       "472    10.0  NaN     98    0.181189    0.932433    0.328949      0.8938  0.5\n",
       "473    10.0  NaN     99    0.179308    0.933333    0.328593      0.8937  0.5\n",
       "\n",
       "[474 rows x 8 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.read_json(json_fname) for json_fname in list_json],ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4305/4179297450.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df9 = df.drop(df.index[0:174],0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>k</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accu</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accu</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.850029</td>\n",
       "      <td>0.687767</td>\n",
       "      <td>0.864808</td>\n",
       "      <td>0.6840</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.694835</td>\n",
       "      <td>0.734950</td>\n",
       "      <td>0.713270</td>\n",
       "      <td>0.7292</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.631026</td>\n",
       "      <td>0.756767</td>\n",
       "      <td>0.649736</td>\n",
       "      <td>0.7509</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.585383</td>\n",
       "      <td>0.776883</td>\n",
       "      <td>0.604490</td>\n",
       "      <td>0.7689</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.550702</td>\n",
       "      <td>0.794150</td>\n",
       "      <td>0.570388</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95</td>\n",
       "      <td>0.181258</td>\n",
       "      <td>0.932333</td>\n",
       "      <td>0.323428</td>\n",
       "      <td>0.8945</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>0.183912</td>\n",
       "      <td>0.931067</td>\n",
       "      <td>0.327432</td>\n",
       "      <td>0.8939</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97</td>\n",
       "      <td>0.182319</td>\n",
       "      <td>0.931750</td>\n",
       "      <td>0.328123</td>\n",
       "      <td>0.8941</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>0.181189</td>\n",
       "      <td>0.932433</td>\n",
       "      <td>0.328949</td>\n",
       "      <td>0.8938</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.179308</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.328593</td>\n",
       "      <td>0.8937</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        n   k  epoch  train_loss  train_accu  valid_loss  valid_accu    p\n",
       "174  10.0 NaN      0    0.850029    0.687767    0.864808      0.6840  0.1\n",
       "175  10.0 NaN      1    0.694835    0.734950    0.713270      0.7292  0.1\n",
       "176  10.0 NaN      2    0.631026    0.756767    0.649736      0.7509  0.1\n",
       "177  10.0 NaN      3    0.585383    0.776883    0.604490      0.7689  0.1\n",
       "178  10.0 NaN      4    0.550702    0.794150    0.570388      0.7833  0.1\n",
       "..    ...  ..    ...         ...         ...         ...         ...  ...\n",
       "469  10.0 NaN     95    0.181258    0.932333    0.323428      0.8945  0.5\n",
       "470  10.0 NaN     96    0.183912    0.931067    0.327432      0.8939  0.5\n",
       "471  10.0 NaN     97    0.182319    0.931750    0.328123      0.8941  0.5\n",
       "472  10.0 NaN     98    0.181189    0.932433    0.328949      0.8938  0.5\n",
       "473  10.0 NaN     99    0.179308    0.933333    0.328593      0.8937  0.5\n",
       "\n",
       "[300 rows x 8 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9 = df.drop(df.index[0:174],0)\n",
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n             10.000000\n",
       "k                   NaN\n",
       "epoch         99.000000\n",
       "train_loss     0.200473\n",
       "train_accu     0.924783\n",
       "valid_loss     0.320957\n",
       "valid_accu     0.892100\n",
       "p              0.100000\n",
       "Name: 273, dtype: float64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9.iloc[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4305/956571061.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df1 = df.drop(\"n\",1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accu</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accu</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.324914</td>\n",
       "      <td>0.879900</td>\n",
       "      <td>0.358151</td>\n",
       "      <td>0.8712</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.318141</td>\n",
       "      <td>0.882533</td>\n",
       "      <td>0.351912</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.312732</td>\n",
       "      <td>0.884967</td>\n",
       "      <td>0.347244</td>\n",
       "      <td>0.8755</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.307159</td>\n",
       "      <td>0.886817</td>\n",
       "      <td>0.343223</td>\n",
       "      <td>0.8769</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.302259</td>\n",
       "      <td>0.888633</td>\n",
       "      <td>0.339378</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>NaN</td>\n",
       "      <td>95</td>\n",
       "      <td>0.181258</td>\n",
       "      <td>0.932333</td>\n",
       "      <td>0.323428</td>\n",
       "      <td>0.8945</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>0.183912</td>\n",
       "      <td>0.931067</td>\n",
       "      <td>0.327432</td>\n",
       "      <td>0.8939</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>NaN</td>\n",
       "      <td>97</td>\n",
       "      <td>0.182319</td>\n",
       "      <td>0.931750</td>\n",
       "      <td>0.328123</td>\n",
       "      <td>0.8941</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>0.181189</td>\n",
       "      <td>0.932433</td>\n",
       "      <td>0.328949</td>\n",
       "      <td>0.8938</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>0.179308</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.328593</td>\n",
       "      <td>0.8937</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>474 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       k  epoch  train_loss  train_accu  valid_loss  valid_accu    p\n",
       "0    0.0      0    0.324914    0.879900    0.358151      0.8712  NaN\n",
       "1    0.0      1    0.318141    0.882533    0.351912      0.8740  NaN\n",
       "2    0.0      2    0.312732    0.884967    0.347244      0.8755  NaN\n",
       "3    0.0      3    0.307159    0.886817    0.343223      0.8769  NaN\n",
       "4    0.0      4    0.302259    0.888633    0.339378      0.8781  NaN\n",
       "..   ...    ...         ...         ...         ...         ...  ...\n",
       "469  NaN     95    0.181258    0.932333    0.323428      0.8945  0.5\n",
       "470  NaN     96    0.183912    0.931067    0.327432      0.8939  0.5\n",
       "471  NaN     97    0.182319    0.931750    0.328123      0.8941  0.5\n",
       "472  NaN     98    0.181189    0.932433    0.328949      0.8938  0.5\n",
       "473  NaN     99    0.179308    0.933333    0.328593      0.8937  0.5\n",
       "\n",
       "[474 rows x 7 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.drop(\"n\",1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>k</th>\n",
       "      <th>p</th>\n",
       "      <th>train_accu</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_accu</th>\n",
       "      <th>valid_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch   k  p  train_accu  train_loss  valid_accu  valid_loss\n",
       "0       0  12  3          15          15          15          15\n",
       "1       1  12  3          15          15          15          15\n",
       "2       2  10  3          13          13          13          13\n",
       "3       3  10  3          13          13          13          13\n",
       "4       4  10  3          13          13          13          13\n",
       "..    ...  .. ..         ...         ...         ...         ...\n",
       "95     95   1  3           4           4           4           4\n",
       "96     96   1  3           4           4           4           4\n",
       "97     97   1  3           4           4           4           4\n",
       "98     98   1  3           4           4           4           4\n",
       "99     99   1  3           4           4           4           4\n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df1.pivot_table(index=[\"epoch\"],aggfunc=\"count\").reset_index()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>k</th>\n",
       "      <th>p</th>\n",
       "      <th>train_accu</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_accu</th>\n",
       "      <th>valid_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.789876</td>\n",
       "      <td>0.564979</td>\n",
       "      <td>0.781433</td>\n",
       "      <td>0.594581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.820467</td>\n",
       "      <td>0.474930</td>\n",
       "      <td>0.810980</td>\n",
       "      <td>0.506263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.844522</td>\n",
       "      <td>0.412610</td>\n",
       "      <td>0.833646</td>\n",
       "      <td>0.446886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.852082</td>\n",
       "      <td>0.394191</td>\n",
       "      <td>0.839823</td>\n",
       "      <td>0.429650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.858094</td>\n",
       "      <td>0.380431</td>\n",
       "      <td>0.845500</td>\n",
       "      <td>0.416974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.930258</td>\n",
       "      <td>0.186333</td>\n",
       "      <td>0.893850</td>\n",
       "      <td>0.320144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.930342</td>\n",
       "      <td>0.185880</td>\n",
       "      <td>0.893925</td>\n",
       "      <td>0.321446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.930583</td>\n",
       "      <td>0.185210</td>\n",
       "      <td>0.894150</td>\n",
       "      <td>0.321978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.931229</td>\n",
       "      <td>0.184161</td>\n",
       "      <td>0.894100</td>\n",
       "      <td>0.322091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.931179</td>\n",
       "      <td>0.184359</td>\n",
       "      <td>0.893650</td>\n",
       "      <td>0.323954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch    k    p  train_accu  train_loss  valid_accu  valid_loss\n",
       "0       0  0.0  0.3    0.789876    0.564979    0.781433    0.594581\n",
       "1       1  0.0  0.3    0.820467    0.474930    0.810980    0.506263\n",
       "2       2  0.0  0.3    0.844522    0.412610    0.833646    0.446886\n",
       "3       3  0.0  0.3    0.852082    0.394191    0.839823    0.429650\n",
       "4       4  0.0  0.3    0.858094    0.380431    0.845500    0.416974\n",
       "..    ...  ...  ...         ...         ...         ...         ...\n",
       "95     95  0.0  0.3    0.930258    0.186333    0.893850    0.320144\n",
       "96     96  0.0  0.3    0.930342    0.185880    0.893925    0.321446\n",
       "97     97  0.0  0.3    0.930583    0.185210    0.894150    0.321978\n",
       "98     98  0.0  0.3    0.931229    0.184161    0.894100    0.322091\n",
       "99     99  0.0  0.3    0.931179    0.184359    0.893650    0.323954\n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df1.pivot_table(index=[\"epoch\"],aggfunc=\"mean\").reset_index()\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axes=plt.subplots(1,2)\n",
    "fig.set_size_inches(10.0,5.0)\n",
    "colors = cm.Dark2.colors\n",
    "for color,p in zip(colors,df[\"p\"].unique()):\n",
    "    dfn = df3[df3[\"p\"]==p]\n",
    "    x = dfn[\"epoch\"]\n",
    "    ax = axes[0]\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.plot(x,dfn[\"train_loss\"],label=f\"train p={p}\",color=color)\n",
    "    ax.plot(x,dfn[\"valid_loss\"],label=f\"valid p={p}\",color=color,linestyle='--')\n",
    "    ax.legend()\n",
    "    ax = axes[1]\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"accuracy\")\n",
    "    ax.plot(x,dfn[\"train_accu\"],label=f\"train p={p}\",color=color)\n",
    "    ax.plot(x,dfn[\"valid_accu\"],label=f\"valid p={p}\",color=color,linestyle='--')\n",
    "    ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4305/1740212885.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df9 = df9.drop(\"k\",1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accu</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accu</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0</td>\n",
       "      <td>0.850029</td>\n",
       "      <td>0.687767</td>\n",
       "      <td>0.864808</td>\n",
       "      <td>0.6840</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1</td>\n",
       "      <td>0.694835</td>\n",
       "      <td>0.734950</td>\n",
       "      <td>0.713270</td>\n",
       "      <td>0.7292</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2</td>\n",
       "      <td>0.631026</td>\n",
       "      <td>0.756767</td>\n",
       "      <td>0.649736</td>\n",
       "      <td>0.7509</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>0.585383</td>\n",
       "      <td>0.776883</td>\n",
       "      <td>0.604490</td>\n",
       "      <td>0.7689</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>4</td>\n",
       "      <td>0.550702</td>\n",
       "      <td>0.794150</td>\n",
       "      <td>0.570388</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>95</td>\n",
       "      <td>0.181258</td>\n",
       "      <td>0.932333</td>\n",
       "      <td>0.323428</td>\n",
       "      <td>0.8945</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>96</td>\n",
       "      <td>0.183912</td>\n",
       "      <td>0.931067</td>\n",
       "      <td>0.327432</td>\n",
       "      <td>0.8939</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>97</td>\n",
       "      <td>0.182319</td>\n",
       "      <td>0.931750</td>\n",
       "      <td>0.328123</td>\n",
       "      <td>0.8941</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>98</td>\n",
       "      <td>0.181189</td>\n",
       "      <td>0.932433</td>\n",
       "      <td>0.328949</td>\n",
       "      <td>0.8938</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>99</td>\n",
       "      <td>0.179308</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.328593</td>\n",
       "      <td>0.8937</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  train_loss  train_accu  valid_loss  valid_accu    p\n",
       "174      0    0.850029    0.687767    0.864808      0.6840  0.1\n",
       "175      1    0.694835    0.734950    0.713270      0.7292  0.1\n",
       "176      2    0.631026    0.756767    0.649736      0.7509  0.1\n",
       "177      3    0.585383    0.776883    0.604490      0.7689  0.1\n",
       "178      4    0.550702    0.794150    0.570388      0.7833  0.1\n",
       "..     ...         ...         ...         ...         ...  ...\n",
       "469     95    0.181258    0.932333    0.323428      0.8945  0.5\n",
       "470     96    0.183912    0.931067    0.327432      0.8939  0.5\n",
       "471     97    0.182319    0.931750    0.328123      0.8941  0.5\n",
       "472     98    0.181189    0.932433    0.328949      0.8938  0.5\n",
       "473     99    0.179308    0.933333    0.328593      0.8937  0.5\n",
       "\n",
       "[300 rows x 6 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9 = df9.drop(\"k\",1)\n",
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>p</th>\n",
       "      <th>train_accu</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_accu</th>\n",
       "      <th>valid_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  p  train_accu  train_loss  valid_accu  valid_loss\n",
       "0       0  3           3           3           3           3\n",
       "1       1  3           3           3           3           3\n",
       "2       2  3           3           3           3           3\n",
       "3       3  3           3           3           3           3\n",
       "4       4  3           3           3           3           3\n",
       "..    ... ..         ...         ...         ...         ...\n",
       "95     95  3           3           3           3           3\n",
       "96     96  3           3           3           3           3\n",
       "97     97  3           3           3           3           3\n",
       "98     98  3           3           3           3           3\n",
       "99     99  3           3           3           3           3\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df10 = df9.pivot_table(index=[\"epoch\"],aggfunc=\"count\").reset_index()\n",
    "df10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>p</th>\n",
       "      <th>train_accu</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_accu</th>\n",
       "      <th>valid_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.668528</td>\n",
       "      <td>0.895194</td>\n",
       "      <td>0.667300</td>\n",
       "      <td>0.907979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.744039</td>\n",
       "      <td>0.666361</td>\n",
       "      <td>0.740333</td>\n",
       "      <td>0.684108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.769833</td>\n",
       "      <td>0.591377</td>\n",
       "      <td>0.764133</td>\n",
       "      <td>0.608447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.788406</td>\n",
       "      <td>0.548727</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.566208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.802994</td>\n",
       "      <td>0.518249</td>\n",
       "      <td>0.793367</td>\n",
       "      <td>0.536692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.929072</td>\n",
       "      <td>0.189726</td>\n",
       "      <td>0.895033</td>\n",
       "      <td>0.311553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.928989</td>\n",
       "      <td>0.189741</td>\n",
       "      <td>0.895267</td>\n",
       "      <td>0.313254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.929089</td>\n",
       "      <td>0.189211</td>\n",
       "      <td>0.895433</td>\n",
       "      <td>0.314080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.929711</td>\n",
       "      <td>0.187853</td>\n",
       "      <td>0.895267</td>\n",
       "      <td>0.314502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.930283</td>\n",
       "      <td>0.186570</td>\n",
       "      <td>0.894900</td>\n",
       "      <td>0.314762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch    p  train_accu  train_loss  valid_accu  valid_loss\n",
       "0       0  0.3    0.668528    0.895194    0.667300    0.907979\n",
       "1       1  0.3    0.744039    0.666361    0.740333    0.684108\n",
       "2       2  0.3    0.769833    0.591377    0.764133    0.608447\n",
       "3       3  0.3    0.788406    0.548727    0.779167    0.566208\n",
       "4       4  0.3    0.802994    0.518249    0.793367    0.536692\n",
       "..    ...  ...         ...         ...         ...         ...\n",
       "95     95  0.3    0.929072    0.189726    0.895033    0.311553\n",
       "96     96  0.3    0.928989    0.189741    0.895267    0.313254\n",
       "97     97  0.3    0.929089    0.189211    0.895433    0.314080\n",
       "98     98  0.3    0.929711    0.187853    0.895267    0.314502\n",
       "99     99  0.3    0.930283    0.186570    0.894900    0.314762\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df11 = df9.pivot_table(index=[\"epoch\"],aggfunc=\"mean\").reset_index()\n",
    "df11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axes=plt.subplots(1,2)\n",
    "fig.set_size_inches(10.0,5.0)\n",
    "colors = cm.Dark2.colors\n",
    "for color,p in zip(colors,df9[\"p\"].unique()):\n",
    "    df11 = df11[df11[\"p\"]==p]\n",
    "    x = df11[\"epoch\"]\n",
    "    ax = axes[0]\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.plot(x,df11[\"train_loss\"],label=f\"train p={p}\",color=color)\n",
    "    ax.plot(x,df11[\"valid_loss\"],label=f\"valid p={p}\",color=color,linestyle='--')\n",
    "    ax.legend()\n",
    "    ax = axes[1]\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"accuracy\")\n",
    "    ax.plot(x,df11[\"train_accu\"],label=f\"train p={p}\",color=color)\n",
    "    ax.plot(x,df11[\"valid_accu\"],label=f\"valid p={p}\",color=color,linestyle='--')\n",
    "    ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
