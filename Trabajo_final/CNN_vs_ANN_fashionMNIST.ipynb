{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "#from torch.utils.tensorboard import SummaryWriter # TensorBoard support\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import time\n",
    "#from torchviz import make_dot\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import dill\n",
    "import json\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)     # On by default, leave it here for clarity\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import files  \n",
    "    COLAB = True\n",
    "except:\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mejores resultados para un ANN clasificador de una capa oculta eran con 512 neuronas de capa oculta y 36 epocas de entrenamiento, mientras que para la CNN son con x epocas de entrenamiento, dropout 0.3, funcion de activacion LeakyReLU y optimizador Adam. Primero entrenamos aca a la ANN en 36 epocas, despues entrenamos a la CNN con x epocas y luego les hacemos el test loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando el dispositivo cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Usando el dispositivo {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La primera vez esto tarda un rato ya que tiene que bajar los datos de la red.\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_orig = train_dataset\n",
    "len(train_dataset_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self,n=512):\n",
    "        super(ANN,self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(28*28,n)\n",
    "        self.linear2 = nn.Linear(n,10)\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network, expand on top of nn.Module\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # define layers\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "    \n",
    "    \n",
    "    self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "    self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "    self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    \n",
    "    self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "  # define forward function\n",
    "  def forward(self, t):\n",
    "    # conv 1\n",
    "    t = self.conv1(t)\n",
    "    t = F.leaky_relu(t)\n",
    "    t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "    # conv 2\n",
    "    t = self.conv2(t)\n",
    "    t = F.leaky_relu(t)\n",
    "    t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "    # fc1\n",
    "    t = t.reshape(-1, 12*4*4)\n",
    "    t = self.fc1(t)\n",
    "    t = self.dropout(t)\n",
    "    t = F.leaky_relu(t)\n",
    "\n",
    "    # fc2\n",
    "    t = self.fc2(t)\n",
    "    t = self.dropout(t)\n",
    "    t = F.leaky_relu(t)\n",
    "\n",
    "    # output\n",
    "    t = self.out(t)\n",
    "    # don't need softmax here since we'll use cross-entropy as activation.\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función de entrenamiento\n",
    "def train_loop(dataloader,model,loss_fn,optimizer,verbose_each=32):  \n",
    "    # Calculamos cosas utiles que necesitamos\n",
    "    num_samples = len(dataloader.dataset) #numero de muestras de entrenamiento\n",
    "    # Seteamos el modelo en modo entrenamiento. Esto sirve para activar, por ejemplo, dropout, etc. durante la fase de entrenamiento.\n",
    "    model.train()\n",
    "    # Pasamos el modelo a la GPU si está disponible.        \n",
    "    model = model.to(device)    \n",
    "    # Iteramos sobre lotes (batchs)\n",
    "    for batch,(X,y) in enumerate(dataloader):\n",
    "        # Pasamos los tensores a la GPU si está disponible.\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)      \n",
    "        # Calculamos la predicción del modelo y la correspondiente pérdida (error)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "        # Backpropagamos usando el optimizador proveido.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Imprimimos el progreso cada 100 batchs\n",
    "        if batch % verbose_each*len(X) == 0:\n",
    "            loss   = loss.item()\n",
    "            sample = batch*len(X) # Número de batch * número de muestras en cada batch\n",
    "            #print(f\"batch={batch} loss={loss:>7f}  muestras-procesadas:[{sample:>5d}/{num_samples:>5d}]\")            \n",
    "# De manera similar, definimos la función de validación y testeo\n",
    "def test_loop(dataloader,model,loss_fn):\n",
    "    num_samples  = 0\n",
    "    num_batches  = 0\n",
    "    avrg_loss    = 0\n",
    "    frac_correct = 0\n",
    "    # Seteamos el modelo en modo evaluacion. Esto sirve para desactivar, por ejemplo, dropout, etc. cuando no estamos en una fase de entrenamiento.\n",
    "    model.eval()\n",
    "    # Pasamos el modelo la GPU si está disponible.    \n",
    "    model = model.to(device)    \n",
    "    # Para validar, desactivamos el cálculo de gradientes.\n",
    "    with torch.no_grad():\n",
    "        # Iteramos sobre lotes (batches)\n",
    "        for X,y in dataloader:\n",
    "            # Pasamos los tensores a la GPU si está disponible.\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)           \n",
    "            # Calculamos las predicciones del modelo...\n",
    "            pred = model(X)\n",
    "            # y las correspondientes pérdidas (errores), los cuales vamos acumulando en un valor total.\n",
    "            num_batches += 1\n",
    "            avrg_loss += loss_fn(pred,y).item()\n",
    "            # También calculamos el número de predicciones correctas, y lo acumulamos en un total.\n",
    "            num_samples += y.size(0)            \n",
    "            frac_correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    # Calculamos la pérdida total y la fracción de clasificaciones correctas, y las imprimimos.\n",
    "    avrg_loss    /= num_batches\n",
    "    frac_correct /= num_samples\n",
    "    #print(f\"Test Error: \\n Accuracy: {frac_correct:>0.5f}, Avg. loss: {avrg_loss:>8f} \\n\")\n",
    "    return avrg_loss,frac_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0 epoch=0 train_loss=0.5448048406839371 train_accu=0.81442 valid_loss=0.5438126623630524 valid_accu=0.8134\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=1 train_loss=0.46033042907714844 train_accu=0.84366 valid_loss=0.46605122089385986 valid_accu=0.8393\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=2 train_loss=0.42169182300567626 train_accu=0.856 valid_loss=0.4303102374076843 valid_accu=0.85\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=3 train_loss=0.3941757494211197 train_accu=0.86516 valid_loss=0.40575324296951293 valid_accu=0.8585\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=4 train_loss=0.37308495104312894 train_accu=0.87116 valid_loss=0.3881243646144867 valid_accu=0.8653\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=5 train_loss=0.355564803481102 train_accu=0.8768 valid_loss=0.3742578595876694 valid_accu=0.8708\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=6 train_loss=0.34098453402519224 train_accu=0.88186 valid_loss=0.3635577201843262 valid_accu=0.8738\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=7 train_loss=0.33407500743865964 train_accu=0.88334 valid_loss=0.3606480032205582 valid_accu=0.8728\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=8 train_loss=0.3305407613515854 train_accu=0.88432 valid_loss=0.36095128059387205 valid_accu=0.8741\n",
      "k=0 epoch=9 train_loss=0.3198802310228348 train_accu=0.88766 valid_loss=0.3534028887748718 valid_accu=0.8767\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=10 train_loss=0.3097065883874893 train_accu=0.89122 valid_loss=0.3463754415512085 valid_accu=0.8787\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=11 train_loss=0.3004265093803406 train_accu=0.8939 valid_loss=0.34050853848457335 valid_accu=0.8794\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=12 train_loss=0.2918146699666977 train_accu=0.89692 valid_loss=0.33499070107936857 valid_accu=0.8816\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=13 train_loss=0.2839143708348274 train_accu=0.89948 valid_loss=0.3299368917942047 valid_accu=0.8824\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=14 train_loss=0.2760977867245674 train_accu=0.90254 valid_loss=0.32555127143859863 valid_accu=0.8846\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=15 train_loss=0.26877708077430723 train_accu=0.90504 valid_loss=0.32109037339687346 valid_accu=0.8862\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=16 train_loss=0.2628541865944862 train_accu=0.90656 valid_loss=0.3185487985610962 valid_accu=0.887\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=17 train_loss=0.25779356896877287 train_accu=0.90832 valid_loss=0.316404315829277 valid_accu=0.8882\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=18 train_loss=0.252873640358448 train_accu=0.91042 valid_loss=0.3147940129041672 valid_accu=0.8889\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=19 train_loss=0.2470739808678627 train_accu=0.91244 valid_loss=0.3119466334581375 valid_accu=0.8902\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=20 train_loss=0.2422572883963585 train_accu=0.91362 valid_loss=0.310441854596138 valid_accu=0.8913\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=21 train_loss=0.23829243570566178 train_accu=0.91536 valid_loss=0.30941930413246155 valid_accu=0.8919\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=22 train_loss=0.2340562665462494 train_accu=0.91688 valid_loss=0.30811276137828825 valid_accu=0.892\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=23 train_loss=0.2294173300266266 train_accu=0.9181 valid_loss=0.30653713941574096 valid_accu=0.8931\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=24 train_loss=0.2254674342274666 train_accu=0.91958 valid_loss=0.30551922619342803 valid_accu=0.8932\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=25 train_loss=0.22210630059242248 train_accu=0.9201 valid_loss=0.3056294322013855 valid_accu=0.8931\n",
      "k=0 epoch=26 train_loss=0.21879012644290924 train_accu=0.92152 valid_loss=0.3052266001701355 valid_accu=0.8928\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=27 train_loss=0.21501779466867446 train_accu=0.92262 valid_loss=0.30448741018772124 valid_accu=0.8938\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=28 train_loss=0.21174220412969588 train_accu=0.92406 valid_loss=0.3040605098009109 valid_accu=0.8941\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=29 train_loss=0.20855044722557067 train_accu=0.92486 valid_loss=0.30445090532302854 valid_accu=0.8928\n",
      "k=0 epoch=30 train_loss=0.2057005375623703 train_accu=0.9261 valid_loss=0.30431661307811736 valid_accu=0.8928\n",
      "k=0 epoch=31 train_loss=0.20125345170497894 train_accu=0.9279 valid_loss=0.30253407061100007 valid_accu=0.894\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=32 train_loss=0.19686202555894852 train_accu=0.92922 valid_loss=0.30169626176357267 valid_accu=0.896\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=33 train_loss=0.1939621365070343 train_accu=0.92994 valid_loss=0.30203895568847655 valid_accu=0.8956\n",
      "k=0 epoch=34 train_loss=0.1886052080988884 train_accu=0.93206 valid_loss=0.2993244618177414 valid_accu=0.897\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=35 train_loss=0.18361166656017303 train_accu=0.93414 valid_loss=0.29726665616035464 valid_accu=0.8979\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=36 train_loss=0.1798126283288002 train_accu=0.93566 valid_loss=0.2966891884803772 valid_accu=0.8979\n",
      "   Saving model_ANN = best-model-2022-02-22-12-41-15.ptm ... DONE!\n",
      "k=0 epoch=37 train_loss=0.17719873666763306 train_accu=0.93656 valid_loss=0.2970786213874817 valid_accu=0.8977\n",
      "k=0 epoch=38 train_loss=0.17692027986049652 train_accu=0.9363 valid_loss=0.29909858405590056 valid_accu=0.8985\n",
      "k=0 epoch=39 train_loss=0.17875932961702345 train_accu=0.93534 valid_loss=0.3035310208797455 valid_accu=0.897\n"
     ]
    }
   ],
   "source": [
    "# Definimos hiperparámetros de entrenamiento\n",
    "init_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1000\n",
    "num_epochs = 40\n",
    "num_k = 1 #72\n",
    "# Recordar que 28*28=784\n",
    "#n=512\n",
    "# Creamos una funcion de perdida\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Creamos un DataFrame de pandas para ir almacenando los valores calculados.\n",
    "dfANN = pd.DataFrame()\n",
    "# Simulamos por tramos porque google colab se desconecta antes de que concluya para todos los valores de n en la lista.\n",
    "min_valid_loss = 10000000.0\n",
    "max_valid_accu = 0.0  \n",
    "for k in range(num_k):\n",
    "    # Creamos el modelo y el optimzador\n",
    "    model = ANN()\n",
    "    # Dividimos el dataset de entrenamiento, el cual tiene 60000 muestras, en 60 partes de 1000 muestras.\n",
    "    train_dataset,valid_dataset = random_split(train_dataset_orig,[50000,10000])\n",
    "    # Creamos los dataloaders ...\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=batch_size)\n",
    "    valid_dataloader = DataLoader(valid_dataset,batch_size=batch_size)         \n",
    "    #optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "    # Entrenamos el modelo y calcualmos curvas.\n",
    "    min_valid_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loop(train_dataloader,model,loss_fn,optimizer)\n",
    "        train_loss,train_accu = test_loop(train_dataloader,model,loss_fn)\n",
    "        valid_loss,valid_accu = test_loop(valid_dataloader,model,loss_fn)\n",
    "        print(f\"k={k} epoch={epoch} train_loss={train_loss} train_accu={train_accu} valid_loss={valid_loss} valid_accu={valid_accu}\")\n",
    "        dfANN = dfANN.append({\"k\":k,\n",
    "                        \"epoch\":epoch,\n",
    "                        \"train_loss\":train_loss,\n",
    "                        \"train_accu\":train_accu,\n",
    "                        \"valid_loss\":valid_loss,\n",
    "                        \"valid_accu\":valid_accu}\n",
    "                        ,ignore_index=True)\n",
    "        if min_valid_loss > valid_loss: # or max_valid_accu < valid_accu:\n",
    "            if min_valid_loss > valid_loss:\n",
    "                min_valid_loss = valid_loss\n",
    "            if max_valid_accu < valid_accu:\n",
    "                max_valid_accu = valid_accu\n",
    "            # Guardamos los parámetros del modelo.\n",
    "            model_ANN = \"best-model-\"+init_datetime+\".ptm\"\n",
    "            print(\"   Saving model_ANN =\",model_ANN,end=\"\")\n",
    "            print(\" ... DONE!\")\n",
    "            torch.save(model.state_dict(),model_ANN)\n",
    "json_ANN = \"simulation-results-\"+init_datetime+\".json\"\n",
    "df.to_json(json_ANN)\n",
    "if COLAB:\n",
    "    files.download(model_ANN)\n",
    "    files.download(json_ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best-model-2022-02-22-12-41-15.ptm'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0 epoch=0 train_loss=0.9371773707866669 train_accu=0.67734 valid_loss=0.9513522267341614 valid_accu=0.6662\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=1 train_loss=0.7006771576404571 train_accu=0.73828 valid_loss=0.7174831211566925 valid_accu=0.732\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=2 train_loss=0.6214777946472168 train_accu=0.76024 valid_loss=0.6356602013111115 valid_accu=0.7541\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=3 train_loss=0.5877949130535126 train_accu=0.77422 valid_loss=0.6012763857841492 valid_accu=0.7681\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=4 train_loss=0.5556639468669892 train_accu=0.78352 valid_loss=0.5680227816104889 valid_accu=0.7787\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=5 train_loss=0.5322651493549347 train_accu=0.79144 valid_loss=0.5453404605388641 valid_accu=0.7879\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=6 train_loss=0.5115898770093917 train_accu=0.7979 valid_loss=0.52566799223423 valid_accu=0.792\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=7 train_loss=0.49590411484241487 train_accu=0.80572 valid_loss=0.5104560077190399 valid_accu=0.8001\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=8 train_loss=0.47897797167301176 train_accu=0.81408 valid_loss=0.4934710294008255 valid_accu=0.8077\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=9 train_loss=0.46941143453121187 train_accu=0.81978 valid_loss=0.48428474068641664 valid_accu=0.8151\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=10 train_loss=0.45498853743076323 train_accu=0.82518 valid_loss=0.4706491053104401 valid_accu=0.8174\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=11 train_loss=0.4439860761165619 train_accu=0.83122 valid_loss=0.46082800924777984 valid_accu=0.8257\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=12 train_loss=0.4324397760629654 train_accu=0.8384 valid_loss=0.4500292748212814 valid_accu=0.8331\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=13 train_loss=0.4245949733257294 train_accu=0.84292 valid_loss=0.44330845177173617 valid_accu=0.8351\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=14 train_loss=0.41042601943016055 train_accu=0.84764 valid_loss=0.4316614717245102 valid_accu=0.8395\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=15 train_loss=0.4012052935361862 train_accu=0.8512 valid_loss=0.4230772227048874 valid_accu=0.8438\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=16 train_loss=0.38773213922977445 train_accu=0.85694 valid_loss=0.40958539843559266 valid_accu=0.847\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=17 train_loss=0.3824638718366623 train_accu=0.85862 valid_loss=0.4071251183748245 valid_accu=0.847\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=18 train_loss=0.37291046142578127 train_accu=0.86422 valid_loss=0.3968497961759567 valid_accu=0.8535\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=19 train_loss=0.36389403641223905 train_accu=0.86526 valid_loss=0.39142243564128876 valid_accu=0.8541\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=20 train_loss=0.3578618156909943 train_accu=0.8692 valid_loss=0.3860307663679123 valid_accu=0.8564\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=21 train_loss=0.3486998975276947 train_accu=0.8722 valid_loss=0.37795233726501465 valid_accu=0.8607\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=22 train_loss=0.34446715235710146 train_accu=0.87332 valid_loss=0.37537225186824796 valid_accu=0.8602\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=23 train_loss=0.3393029659986496 train_accu=0.87518 valid_loss=0.3708801090717316 valid_accu=0.8615\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=24 train_loss=0.33293077766895296 train_accu=0.87744 valid_loss=0.36531549990177153 valid_accu=0.8659\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=25 train_loss=0.32410522758960725 train_accu=0.88082 valid_loss=0.35700797140598295 valid_accu=0.8654\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=26 train_loss=0.3189924985170364 train_accu=0.88256 valid_loss=0.35295998156070707 valid_accu=0.8689\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=27 train_loss=0.3190031623840332 train_accu=0.88188 valid_loss=0.3547935366630554 valid_accu=0.8682\n",
      "k=0 epoch=28 train_loss=0.31279033303260806 train_accu=0.88488 valid_loss=0.34844752550125124 valid_accu=0.8712\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=29 train_loss=0.3072952997684479 train_accu=0.88664 valid_loss=0.3447441965341568 valid_accu=0.8704\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=30 train_loss=0.30499034583568574 train_accu=0.88748 valid_loss=0.3436578422784805 valid_accu=0.8712\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=31 train_loss=0.30058091700077055 train_accu=0.88896 valid_loss=0.3397533506155014 valid_accu=0.8729\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=32 train_loss=0.3004090964794159 train_accu=0.88982 valid_loss=0.3410380184650421 valid_accu=0.8722\n",
      "k=0 epoch=33 train_loss=0.29361418783664706 train_accu=0.89182 valid_loss=0.3350085556507111 valid_accu=0.8748\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=34 train_loss=0.29593383610248564 train_accu=0.89086 valid_loss=0.33739273250102997 valid_accu=0.8738\n",
      "k=0 epoch=35 train_loss=0.28747397601604463 train_accu=0.89402 valid_loss=0.33036676347255706 valid_accu=0.8774\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=36 train_loss=0.2859064197540283 train_accu=0.8952 valid_loss=0.3284962326288223 valid_accu=0.8769\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=37 train_loss=0.284465596973896 train_accu=0.8952 valid_loss=0.3321454733610153 valid_accu=0.8779\n",
      "k=0 epoch=38 train_loss=0.28017994433641435 train_accu=0.89674 valid_loss=0.3272474199533463 valid_accu=0.8793\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=39 train_loss=0.27803500175476076 train_accu=0.8974 valid_loss=0.3257262885570526 valid_accu=0.8789\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=40 train_loss=0.27544629603624343 train_accu=0.89922 valid_loss=0.3256413727998734 valid_accu=0.8792\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=41 train_loss=0.2727105340361595 train_accu=0.90032 valid_loss=0.32379490733146665 valid_accu=0.8794\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=42 train_loss=0.2683983531594276 train_accu=0.90198 valid_loss=0.3185566902160645 valid_accu=0.8814\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=43 train_loss=0.27056817948818207 train_accu=0.9005 valid_loss=0.3198069423437119 valid_accu=0.8788\n",
      "k=0 epoch=44 train_loss=0.26464199513196945 train_accu=0.90232 valid_loss=0.31542757749557493 valid_accu=0.8839\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=45 train_loss=0.25939962148666385 train_accu=0.90412 valid_loss=0.314055809378624 valid_accu=0.8854\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=46 train_loss=0.26178094804286955 train_accu=0.9039 valid_loss=0.31351363360881807 valid_accu=0.8853\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=47 train_loss=0.2570675867795944 train_accu=0.90508 valid_loss=0.3114283740520477 valid_accu=0.8866\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=48 train_loss=0.2562034365534782 train_accu=0.90628 valid_loss=0.31130316853523254 valid_accu=0.8871\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n",
      "k=0 epoch=49 train_loss=0.25312419325113295 train_accu=0.90584 valid_loss=0.3082540214061737 valid_accu=0.8866\n",
      "   Saving model_CNN = best-model-2022-02-22-12-59-04.ptm ... DONE!\n"
     ]
    }
   ],
   "source": [
    "# Definimos hiperparámetros de entrenamiento\n",
    "init_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1000\n",
    "num_epochs = 50\n",
    "num_k = 1 #72\n",
    "# Recordar que 28*28=784\n",
    "#n=512\n",
    "# Creamos una funcion de perdida\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Creamos un DataFrame de pandas para ir almacenando los valores calculados.\n",
    "dfCNN = pd.DataFrame()\n",
    "# Simulamos por tramos porque google colab se desconecta antes de que concluya para todos los valores de n en la lista.\n",
    "min_valid_loss = 10000000.0\n",
    "max_valid_accu = 0.0  \n",
    "for k in range(num_k):\n",
    "    # Creamos el modelo y el optimzador\n",
    "    model = CNN()\n",
    "    # Dividimos el dataset de entrenamiento, el cual tiene 60000 muestras, en 60 partes de 1000 muestras.\n",
    "    train_dataset,valid_dataset = random_split(train_dataset_orig,[50000,10000])\n",
    "    # Creamos los dataloaders ...\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=batch_size)\n",
    "    valid_dataloader = DataLoader(valid_dataset,batch_size=batch_size)         \n",
    "    #optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "    # Entrenamos el modelo y calcualmos curvas.\n",
    "    min_valid_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loop(train_dataloader,model,loss_fn,optimizer)\n",
    "        train_loss,train_accu = test_loop(train_dataloader,model,loss_fn)\n",
    "        valid_loss,valid_accu = test_loop(valid_dataloader,model,loss_fn)\n",
    "        print(f\"k={k} epoch={epoch} train_loss={train_loss} train_accu={train_accu} valid_loss={valid_loss} valid_accu={valid_accu}\")\n",
    "        dfCNN = dfCNN.append({\"k\":k,\n",
    "                        \"epoch\":epoch,\n",
    "                        \"train_loss\":train_loss,\n",
    "                        \"train_accu\":train_accu,\n",
    "                        \"valid_loss\":valid_loss,\n",
    "                        \"valid_accu\":valid_accu}\n",
    "                        ,ignore_index=True)\n",
    "        if min_valid_loss > valid_loss: # or max_valid_accu < valid_accu:\n",
    "            if min_valid_loss > valid_loss:\n",
    "                min_valid_loss = valid_loss\n",
    "            if max_valid_accu < valid_accu:\n",
    "                max_valid_accu = valid_accu\n",
    "            # Guardamos los parámetros del modelo.\n",
    "            model_CNN = \"best-model-\"+init_datetime+\".ptm\"\n",
    "            print(\"   Saving model_CNN =\",model_CNN,end=\"\")\n",
    "            print(\" ... DONE!\")\n",
    "            torch.save(model.state_dict(),model_CNN)\n",
    "json_CNN = \"simulation-results-\"+init_datetime+\".json\"\n",
    "df.to_json(json_CNN)\n",
    "if COLAB:\n",
    "    files.download(model_CNN)\n",
    "    files.download(json_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best-model-2022-02-22-12-59-04.ptm'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ANN = model_ANN.split()[0]\n",
    "model_CNN = model_CNN.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = ANN()\n",
    "modelA.load_state_dict(torch.load(model_ANN,map_location=\"cpu\"))\n",
    "modelA.eval()\n",
    "modelA = modelA.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelC = CNN()\n",
    "modelC.load_state_dict(torch.load(model_CNN,map_location=\"cpu\"))\n",
    "modelC.eval()\n",
    "modelC = modelC.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss ANN =  0.3300893396139145\n",
      "test_accu ANN =  0.8852\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size)\n",
    "test_loss,test_accu = test_loop(test_loader,modelA,loss_fn)\n",
    "print(\"test_loss ANN = \",test_loss)\n",
    "print(\"test_accu ANN = \",test_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss CNN =  0.31221417188644407\n",
      "test_accu CNN =  0.8879\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size)\n",
    "test_loss,test_accu = test_loop(test_loader,modelC,loss_fn)\n",
    "print(\"test_loss CNN = \",test_loss)\n",
    "print(\"test_accu CNN = \",test_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
